<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zheng.H</title>
    <link>https://zhenghuazx.github.io/hua.zheng/</link>
      <atom:link href="https://zhenghuazx.github.io/hua.zheng/index.xml" rel="self" type="application/rss+xml" />
    <description>Zheng.H</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Hua Zheng ©2022</copyright><lastBuildDate>Tue, 11 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://zhenghuazx.github.io/hua.zheng/media/icon_hu658dc7ef748f26cdf7d87fd659137d7d_44050_512x512_fill_lanczos_center_3.png</url>
      <title>Zheng.H</title>
      <link>https://zhenghuazx.github.io/hua.zheng/</link>
    </image>
    
    <item>
      <title>Policy Optimization in Bayesian Network Hybrid Models of Biomanufacturing Processes</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-policy/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-policy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Variance Reduction based Experience Replay for Policy Optimization</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-green/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-green/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Experience replay allows agents to remember and reuse historical transitions. However, the uniform reuse strategy regardless of their significance is implicitly biased toward out-of-date observations. To overcome this limitation, we propose a general variance reduction based experience reply (VRER) approach, which allows policy optimization algorithms to selectively reuse the most relevant samples and improve policy gradient estimation. It tends to put more weight on historical observations that are more likely sampled from the target distribution. Different from other ER methods VRER is a theoretically justified and simple-to-use approach. Our theoretical and empirical studies demonstrate that the proposed VRER can accelerate the learning of optimal policy and enhance the performance of state-of-the-art policy optimization approaches.Experience replay allows agents to remember and reuse historical transitions. However, the uniform reuse strategy regardless of their significance is implicitly biased toward out-of-date observations. To overcome this limitation, we propose a general variance reduction based experience reply (VRER) approach, which allows policy optimization algorithms to selectively reuse the most relevant samples and improve policy gradient estimation. It tends to put more weight on historical observations that are more likely sampled from the target distribution. Different from other ER methods VRER is a theoretically justified and simple-to-use approach. Our theoretical and empirical studies demonstrate that the proposed VRER can accelerate the learning of optimal policy and enhance the performance of state-of-the-art policy optimization approaches.&lt;/p&gt;
&lt;h2 id=&#34;open-source-library-vrer-pg&#34;&gt;Open-source Library: vrer-pg&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;vrer-pg&lt;/strong&gt; is a tensorflow based AI library which facilitates experimentation with existing policy optimization algorithms with variance reduction based experience replay. It provides well tested components that can be easily modified or extended. The available selection of algorithms can be used directly or through command line.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;feature.jpg&#34;  /&gt;
  &lt;figcaption&gt;
      &lt;h4&gt;vrer-pg&lt;/h4&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The performance improvement of state-of-the-art PO algorithms after using VRER. Results are described by the mean performance curves and 95% confidence intervals of PPO(-VRER), TRPO(-VRER) and VPG(-VRER).&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;convergenec-VPG.png&#34;  /&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;convergenec-PPO.png&#34;  /&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;convergenec-TRPO.png&#34;  /&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Some Gaps Between Reinforcement Learning Practice and Theory</title>
      <link>https://zhenghuazx.github.io/hua.zheng/post/somegapsrl/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/post/somegapsrl/</guid>
      <description>&lt;p&gt;I am recently encountering some theoretical problems during a reinforcement learning (RL) research project. I started to realize how much difference between how people use versus how people prove RL algorithms. Although rarely mentioned in the literature, some gaps indeed exist and may be useful to think a little deeper. It motivates me to write some of my thoughts down.&lt;/p&gt;
&lt;p&gt;Note: Some of the practical heuristics might already have theoretical justifications that I do not know or bypassed by some reasonable assumptions. Thus, please correct me if I misunderstand or have mistakes.&lt;/p&gt;
&lt;h1 id=&#34;experience-replay-cause-bias&#34;&gt;Experience Replay Cause Bias&lt;/h1&gt;
&lt;p&gt;Off-policy learning is a large category of RL algorithms. it, instead of following the target policy, enables a target policy to be learned by using data from behavior policies. Experience replay is central to off-policy algorithms in deep reinforcement learning (RL). As the fundamental data-generating mechanism in off-policy deep reinforcement learning, it has been shown to improve sample efficiency and stability by storing and randomly replaying a fixed number of the most recently collected transitions for training.&lt;/p&gt;
&lt;p&gt;One interesting problem of experience replay is that the stored transitions are inevitably dependent with the current/target policy or value function, because the policy/value function parameter is in fact constructed from those transitions. In other words, all transitions are correlated. It &lt;strong&gt;violates&lt;/strong&gt; the following conditional unbiasedness assumption in stochastic policy gradient methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The estimated gradient is unbiased condition on the history (filtration) $\mathcal{F}_k=\{\xi_1,\ldots,\xi_{k};\theta_0,\theta_1,\ldots,\theta_{k}\}$, such that $\mathbb{E}_\xi[g(\theta_k,\xi)|\mathcal{F}_{k}]=\nabla f(\theta_k)$. ($\mathcal{F}_{k}$ denotes the outcomes up to and including time $k$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the classical stochastic gradient descent (SGD), the data are assumed to be sampled from a unknown population, which is also referred as to the behavior distribution in off-policy batch RL. However, when you have a growing experience replay buffer in which transitions were continuously collected by following past policies, you won’t have a fixed unknown behavior distribution but a changing known one. The main concerns behind this conditional dependence makes theoretical study of experience replay difficult.&lt;/p&gt;
&lt;h1 id=&#34;compatibility-of-critic-model&#34;&gt;Compatibility of Critic Model&lt;/h1&gt;
&lt;p&gt;Compatibility is an important concept in actor-critic methods, which are a popular class of on-policy RL algorithms for computing an optimal policy. The actor-critic consists of two eponymous components. An actor adjusts the parameters of the stochastic policy by stochastic gradient ascent. A critic estimates the action-value function to evaluate the action chosen by current policy. Instead of the unknown true action-value function, an action-value function is used, which introduce the concept of compatibility.&lt;/p&gt;
&lt;p&gt;In general, substituting a function approximator for the true action-value (or value) function may introduce bias. However, the bias is zero if the function approximator is compatible, i.e.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the function approximator is linear in the score function of policy&lt;/li&gt;
&lt;li&gt;parameters of the function approximator are chosen to minimize the mean-squared error of approximated and true action-value function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In practice, condition 2 is usually relaxed in favor of policy evaluation algorithms that estimate the value function more efficiently by temporal-difference learning. However, condition 1 is rarely satisfied. This issues applies to almost all, if not all, modern deep policy optimization algorithms.&lt;/p&gt;
&lt;p&gt;Good news is that people has released this issue and some literature are working on relaxing the compatibility constraint for neural network [1-3].&lt;/p&gt;
&lt;h1 id=&#34;discounted-objective&#34;&gt;Discounted Objective&lt;/h1&gt;
&lt;p&gt;I had recently revisited the policy gradient theorem proof when working on my actor-critic related paper, which leads to this question.&lt;/p&gt;
&lt;p&gt;After a quick literature review I found Russo Alessio mentioned the same issue in his blog [4] and Nota and Thomas [5] had raised concerns about the similar issue by constructing a counterexample. In addition, Naik et al [6] criticize discounted objective as well but in a different context.
I personally used a lot of discounted objective, but still felt unclear on some concepts. Part of the reason comes from inaccurate or ambiguous description and the other part I think comes from the fact that RL is an interdisciplinary area including contribution from optimization, operations research, statistics and computer science.
As a result, people used different terminologies and notations, which further exaggerates the ambiguity.&lt;/p&gt;
&lt;p&gt;Now let’s briefly discuss the issue in infinite-horizon MDP setting. For detailed discussion, I recommend reading the blog and literature mentioned above.&lt;/p&gt;
&lt;p&gt;In the classic policy gradient theorem, the discounted policy gradient, tells us how to modify the policy parameters, $\theta$, in order to increase $\nabla J_\gamma(\theta)$ , and is given by:&lt;/p&gt;

$$
\begin{equation}
\begin{split}
\nabla J_\gamma(\theta)
&amp;= \mathbb{E}_{s_t\sim d^\pi_\gamma,a_t\sim \pi(s_t|a_t)}\left[\left. \nabla \log\pi_\theta(a_t|s_t)Q^\pi_\gamma(s_t,a_t)\right|\theta\right]
\end{split}
\end{equation}
$$

&lt;p&gt;where $d^\pi_\gamma(s)=\int p(s_0)\sum^\infty_{t=0}\gamma^t p^\pi(s_t=s|s_0)d s_0$ is the discounted stationary state distribution (discounted state occupation).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: the samples used to update the policy should be distributed according to the discounted state distribution $d^\pi_\gamma(s)$ however, the sample actually used in most algorithm are $s_{t+1}\sim p(s_{t+1}|s_t,a_t),a_t\sim \pi(s_t|a_t)$. In average return case, it is allowed because we can reach the limiting distribution by continuously running the simulation, i.e. $d^\pi(s)=\lim_{t\rightarrow \infty}p(s_{t}=s|s_0)$. But in the discounted case, we can&amp;rsquo;t naively take average of samples from reply buffer as the discounted state stationary distribution treat observations sampled at different time steps differently.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Wang, D., &amp;amp; Hu, M. (2021). Deep Deterministic Policy Gradient With Compatible Critic Network. &lt;em&gt;IEEE Transactions on Neural Networks and Learning Systems&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Diddigi, R. B., Jain, P., &amp;amp; Bhatnagar, S. (2021). Neural Network Compatible Off-Policy Natural Actor-Critic Algorithm. &lt;em&gt;arXiv preprint arXiv:2110.10017&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Balduzzi, D., &amp;amp; Ghifary, M. (2015). Compatible value gradients for reinforcement learning of continuous deep policies. &lt;em&gt;arXiv preprint arXiv:1509.03005&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Russo Alessio. (2021) Why there is a problem with the Policy Gradient theorem in Deep Reinforcement Learning. &lt;a href=&#34;https://towardsdatascience.com/why-there-is-a-problem-with-the-policy-gradient-theorem-in-deep-reinforcement-learning-958d845218f1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/why-there-is-a-problem-with-the-policy-gradient-theorem-in-deep-reinforcement-learning-958d845218f1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chris Nota and Philip S. Thomas. 2020. Is the Policy Gradient a Gradient?. AAMAS (2020).&lt;/li&gt;
&lt;li&gt;Naik, A., Shariff, R., Yasui, N., Yao, H., &amp;amp; Sutton, R. S. (2019). Discounted reinforcement learning is not an optimization problem. NeurIPS 2019.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Bayesian Network Based Hybrid Process Model</title>
      <link>https://zhenghuazx.github.io/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/</guid>
      <description>&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#process-modeling&#34;&gt;Process Modeling&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#Linear-Gaussian-Dynamic-Bayesian-Network-Based-Hybrid-Model-4-httpsarxivorgpdf210506543pdf&#34;&gt;Linear Gaussian Dynamic Bayesian Network Based Hybrid Model&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#process-trajectoryepisode-distribution&#34;&gt;Process Trajectory/Episode Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-formulation&#34;&gt;Model Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nonlinear-dynamic-bayesian-network-based-hybrid-modelh6ttpsarxivorgpdf220103116pdf&#34;&gt;Nonlinear Dynamic Bayesian Network Based Hybrid Model&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#parametric-variation-modeling&#34;&gt;Parametric Variation Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-state-modeling&#34;&gt;Latent State Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inference-a-bayesian-approach&#34;&gt;Inference (A Bayesian Approach)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#next&#34;&gt;Next?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This is a tutorial about a series of advanced hybrid process models for people with some background in mathematical and statistical modeling and interested in the process modeling. The class of process models I will present in this tutorial is mainly based on Bayesian network and Bayesian inference.&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;For many biochemical/physical/economic processes, the laws of physics and biochemistry often allow us to construct mechanistic models in forms of ordinary/partial/algebraic differential equations. While these models can well describe the the mean changes in a reaction/movement/economy profile over time, these models often ignore the impact from various sources of process inherent stochasticity. For example, in a cell culture process, although batch-to-batch variation and bioprocess noise are often dominant sources of process variation [1], biochemical kinetics literature rarely incorporates them into the ordinary/partial  differential equation (ODE/PDE) based mechanistic models. In a study on the microbial cell-to-cell phenotypic diversity, [2] identified the intracellular production fluctuations as one of the major sources of the bioprocessing noise. Raw material variability is another critical source of uncertainty impacting cell cultures [3]. In fact, process noises are widely seen in most complex systems such as economy, cell metabolism, production process of mRNA vaccines. Therefore, it is of great importance to understand  and incorporate major sources of stochastic uncertainty into a process model.&lt;/p&gt;
&lt;h1 id=&#34;process-modeling&#34;&gt;Process Modeling&lt;/h1&gt;
&lt;p&gt;Process-based modeling is a modeling framework for complex systems. In contrast to agent-based models and other mainstream languages in complex systems, in a process-based model the structural features of a system are encoded in the interactions between the entities, rather than in the entities themselves.&lt;/p&gt;
&lt;h2 id=&#34;ordinary-differential-equations-based-mechanistic-model&#34;&gt;Ordinary Differential Equations Based Mechanistic Model&lt;/h2&gt;
&lt;p&gt;A control system is a &lt;a href=&#34;http://www.scholarpedia.org/article/Dynamical_Systems&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dynamical system&lt;/a&gt; on which one can act by using suitable controls. In this article, the dynamical model is modeled by ordinary differential equations of the following type&lt;/p&gt;
&lt;p&gt;
$$
\dot{\pmb{s}}=\pmb f(\pmb s,\pmb a)\tag{1}\label{eq: 1}
$$

where $\pmb{f}(\cdot)$ encodes the causal interdependencies between actions and state. One typically assumes that the functional form of $\pmb{f}$ is known, though it may also depend on additional parameters calibrated from data. The variable $\pmb s$ is the state and belongs to some space $\mathcal{S}$. The variable $\pmb{a}$ is the control and belongs to some space $\mathcal{A}$. In this article, the space $\mathcal{S}$ is of infinite dimension.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In control literature, people use the state and control variables $(x,u)$ instead of state and action $(\pmb{s},\pmb{a})$.&lt;/li&gt;
&lt;li&gt;This is a first order differential equation;&lt;/li&gt;
&lt;li&gt;$\pmb f(\cdot)=\left(f_1,\ldots, f_n\right)(\cdot)=\left(f_1(\cdot),\ldots, f_n(\cdot)\right)$ is a vector of functions.&lt;/li&gt;
&lt;li&gt;Some characteristics of the dynamical system can be used to analyze the Eq.(1), such as
&lt;ul&gt;
&lt;li&gt;Stability: means that the trajectories do not change too much under small perturbations.&lt;/li&gt;
&lt;li&gt;Asymptotically stability&lt;/li&gt;
&lt;li&gt;controllability:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: the kinetic/dynamic equations is linear, e.g. $\dot{s}=\mu s(t)$, where $\mu$ is a constant,  and in this case, $f(x)=ax$. The stability of linear systems is straightforward, i.e. stable if $\mu&amp;lt;0$, and unstable otherwise.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;linear-gaussian-dynamic-bayesian-network-based-hybrid-model-4httpsarxivorgpdf210506543pdf&#34;&gt;Linear Gaussian Dynamic Bayesian Network Based Hybrid Model [&lt;a href=&#34;https://arxiv.org/pdf/2105.06543.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;4&lt;/a&gt;]&lt;/h2&gt;
&lt;p&gt;We model the  process dynamics as a finite-horizon Markov decision process (MDP) specified by $(\mathcal{S}, \mathcal{A}, H, r, p)$, where $\mathcal{S}$, $\mathcal{A}$, $H$, $r$ and $p$ represent state space, action space, planning horizon, reward function, and state transition probability model. The process state transition is modeled as,

$$
\begin{equation}
\pmb{s}_{t+1} \sim p(\pmb{s}_{t+1}|\pmb{s}_t,\pmb{a}_t;\pmb\theta_t)
\end{equation}
$$
&lt;/p&gt;
&lt;p&gt;where $\pmb{s}_t\in \mathcal{S}\subset \mathbb{R}^n$ denotes the process state (e.g., glucose and lactate concentrations and cell density), $\pmb{a}_t \in \mathcal{A}\subset\mathbb{R}^m$  is the action (also known as control inputs) at time step $t\in\mathcal{H}$. Here $\mathcal{H}\equiv{1,2,\ldots,H}$ denotes the discrete time index (a.k.a. decision epochs).&lt;/p&gt;
&lt;h3 id=&#34;process-trajectoryepisode-distribution&#34;&gt;Process Trajectory/Episode Distribution&lt;/h3&gt;
&lt;p&gt;Then, the distribution of the entire trajectory $\pmb\tau=(\pmb{s}_1,\pmb{a}_1,\pmb{s}_2,\pmb{a}_2,\ldots,\pmb{s}_{H})$
of the process can be written as a product&lt;/p&gt;
&lt;p&gt;
$$
p(\pmb{\tau}) = p(\pmb{s}_1)\prod_{t=1}^{H-1} p(\pmb{s}_{t+1}|\pmb{s}_t,\pmb{a}_t) p(\pmb{a}_t)\tag{2}\label{eq:trajectory-distribution}
$$

of conditional distributions. Given a set of real-world process observations denoted by $\mathcal{D} =\left\{\pmb\tau^{(n)}\right\}^N_{n=1}$, we quantify the model parameter estimation uncertainty using the posterior distribution $p(\pmb{\theta}|\mathcal{D})$.&lt;/p&gt;
&lt;h3 id=&#34;model-formulation&#34;&gt;Model Formulation&lt;/h3&gt;
&lt;p&gt;Suppose that $\pmb s_t$ evolves according to the ordinary differential equation  \eqref{eq: 1}  and the dynamics is monitored on a small time scale using sensors, let us replace Eq. \eqref{eq: 1}  by the first-order Taylor approximation&lt;/p&gt;
&lt;p&gt;
$$
\begin{equation}
\frac{\Delta \pmb{s}_{t+1}}{\Delta t} =\pmb{f}(\pmb\mu_t^s,\pmb\mu_t^a)+J^s_f(\pmb\mu_t^s)(\pmb{s}_t-\pmb\mu_t^s)+J^a_f(\pmb\mu_t^a)(\pmb{a}_t-\pmb\mu_t^a), \tag{3}\label{eq:taylor1}
\end{equation}
$$

where $\Delta \pmb{s}_{t+1}=\pmb{s}_{t+1}-\pmb{s}_{t}$, and  $J^s_f,J^a_f$ denote the Jacobian matrices of $\pmb{f}$  with respect to $\pmb{s}_t$ and $\pmb{a}_t$, respectively. The interval $\Delta t$ can change with time, but we keep it constant for simplicity. The approximation is taken at a point $\left(\pmb\mu_t^s,\pmb\mu_t^a\right)$. We can then rewrite Eq. \eqref{eq:taylor1}  as&lt;/p&gt;

$$
\begin{equation}
\scriptsize
\pmb{s}_{t+1} = \pmb{\mu}_{t}^s + \Delta t\cdot \pmb{f}(\pmb\mu_t^s,\pmb\mu_t^a)+(\Delta t \cdot J_f(\pmb\mu_t^s)+1)(\pmb{s}_t-\pmb\mu_t^s)+\Delta t \cdot J_f(\pmb\mu_t^a)(\pmb{a}_t-\pmb\mu_t^a)+R_{t+1}\tag{4}\label{eq:taylor}
\end{equation}
$$

&lt;p&gt;where $R_{t+1}$ is a remainder term modeling the effect from other uncontrolled factors. In this way, the original process dynamics have been linearized, with $R_{t+1}$ serving as a residual. One can easily represent Eq. \eqref{eq:taylor}  using a network model. An edge exists from $s^k_t$ (respectively, $a^k_t$) to $s^l_{t+1}$ if the $\left(k,l\right)$-th entry of $J^s_f$ (respectively, $J^a_f$) is not identically zero.&lt;/p&gt;
&lt;p&gt;As will be seen shortly, the linearized dynamics can be rewritten in a linear Gaussian dynamic Bayesian network. Specifically, let&lt;/p&gt;

$$
\begin{align*}\pmb\mu_{t+1}^s &amp;=\pmb{\mu}_{t}^s + \Delta t\cdot \pmb{f}(\pmb\mu_t^s,\pmb\mu_t^a),\\
\pmb\beta_{t}^s &amp;= \Delta t \cdot J_f(\pmb\mu_t^s)+1,\\
\pmb\beta_{t}^a &amp;=\Delta t \cdot J_f(\pmb\mu_t^a),\end{align*}
$$

&lt;p&gt;and treat $R_{t+1}$ as the residual $\pmb{e}_{t+1}=V_{t+1}^{1/2}\pmb{z}$,  with $\pmb{z}$ is an $n$-dimensional standard normal random vector, and $V_{t+1}\triangleq\text{diag}(v_{t+1}^{k})$ is a diagonal matrix of residual standard deviation.  we can turn  Eq. \eqref{eq:taylor}  into a Linear Gaussian (Dynamic) Bayesian Network [5]:&lt;/p&gt;
&lt;p&gt; 
$$
\begin{equation}
\pmb{s}_{t+1} = \pmb{\mu}_{t+1}^s + \left(\pmb{\beta}_{t}^s\right)^\top(\pmb{s}_t-\pmb\mu_t^s) + \left(\pmb\beta_{t}^a\right)^\top(\pmb{a}_t-\pmb\mu_t^a) +V_{t+1}^{1/2}\pmb{z}_{t+1}\tag{5}\label{eq: 5}
\end{equation}
$$

where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; $s_t\in\mathcal{S}\subset \mathbb{R}^n, \pmb{a}_t\in\mathcal{A}\subset \mathbb{R}^{m}$&lt;/li&gt;
&lt;li&gt; $\pmb\mu_{t}^s=(\mu_t^{1},\ldots, \mu_t^n)$ and  $\pmb\mu_{t}^a=(\lambda_t^{1},\ldots, \lambda_t^m)$ ,&lt;/li&gt;
&lt;li&gt; $\pmb\beta_{t}^s$ is the $n\times n$ matrix whose $\left(j,k\right)$-th element is the linear coefficient $\beta^{jk}_t$  corresponding to the effect of state $s^j_t$ on the next state $s^k_{t+1}$&lt;/li&gt;
&lt;li&gt;$\pmb\beta_t^a$ is the $m\times n$ matrix of analogous coefficients representing the effects of each component of $\pmb{a}_t$ on each component of $\pmb{s}_{t+1}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;State transition&lt;/strong&gt;: From Eq.\eqref{eq: 5}, the state transition becomes

$$
\begin{equation*}\small
\pmb{s}_{t+1} \sim p(\pmb{s}_{t+1}|\pmb{s}_t,\pmb{a}_t;\pmb\theta_t)=\mathcal{N}\left( \pmb{\mu}_{t+1}^s + \left(\pmb{\beta}_{t}^s\right)^\top(\pmb{s}_t-\pmb\mu_t^s) + \left(\pmb\beta_{t}^a\right)^\top(\pmb{a}_t-\pmb\mu_t^a), V_{t+1}\right)
\end{equation*}
$$

with $\pmb\theta_t=(\pmb{\mu}_{t+1}^s,\pmb\mu_t^s,\pmb\mu_t^a,\pmb{\beta}_{t}^s, \pmb\beta_{t}^a, V_{t+1})$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initial State:&lt;/strong&gt; $\pmb{s}_1\sim \mathcal{N}(\pmb\mu^s_1,V_1)$ or $s_{1}^k \sim \mathcal{N}(\mu^k_{1},(v^k_{1})^2)$ for $k=1,2,\ldots,m$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; $\pmb{a}_t\sim\mathcal{N}\left(\pmb\mu^a_t,\text{diag}(\sigma_t^k)\right)$ or $a_{t}^k \sim \mathcal{N}(\lambda^k_{t},(\sigma^k_{t})^2)$ for $k=1,2,\ldots,m$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; Letting $\pmb\sigma_t=(\sigma_t^1,\ldots,\sigma_t^m)$ and $\pmb{v}_t=(v_t^1,\ldots,v_t^n)$&lt;em&gt;,&lt;/em&gt; the list of parameters for full model is

$$
\pmb{\theta} = (\pmb{\mu}^s,\pmb\mu^a,\pmb{\beta},\pmb\sigma,\pmb{v})= \{(\pmb{\mu}_{t}^s,\pmb\mu_t^a,\pmb{\beta}_{t}^s,\pmb\beta_{t}^a,\pmb\sigma_t,\pmb{v}_t)| 0\leq t\leq H\}
$$

where $\pmb\beta = (\pmb\beta^a,\pmb\beta^s)$. Next, we will discuss how to estimate parameters $\pmb{\theta}$ from data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\pmb{v}_t$ is the diagonal (vector)of the diagonal covariance matrix $V_t$, i.e. $\pmb{v}_t=\text{diag}(V_{t})$.&lt;/li&gt;
&lt;li&gt;Action can be modeled by a policy distribution $\pmb{a}_t\sim \pi(\pmb{a}_t|\pmb{s}_t)$. This policy $\pi$ can be obtained from RL algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bayesian Approach&lt;/strong&gt; (See details in &lt;strong&gt;Appendix Section 10 of [&lt;a href=&#34;https://arxiv.org/pdf/2105.06543.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;4&lt;/a&gt;]&lt;/strong&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Full likelihood and conjugate prior&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given the data $\mathcal{D}$, the posterior distribution of $\pmb{\theta}$ is proportional to
$\pmb{\theta}$$p(\pmb\theta|\mathcal{D})\propto p\left(\pmb{\theta}\right)\prod_{n=1}^Np\left(\pmb{\tau}^{(n)}|\pmb{\theta}\right)$$.&lt;/p&gt;
&lt;p&gt;The Gibbs sampling technique can be used to sample from this distribution.&lt;/p&gt;
&lt;p&gt;For each node $X$ in the network, let $\text{Ch}\left(X\right)$ be the set of child nodes (direct successors) of $X$. The &lt;strong&gt;full likelihood&lt;/strong&gt; $p\left(\mathcal{D}|\pmb{\theta}\right)$ becomes&lt;/p&gt;

    $$
    \begin{equation*}\scriptsize
    p(\mathcal{D}|\pmb{\theta})=\prod^N_{i=1}p(\pmb{\tau}_i|\pmb{\theta})=\prod^N_{i=1}\prod_{t=1}^H\left[\prod_{k=1}^{m}\mathcal{N}\left(\lambda^k_{t},(\sigma^k_{t})^2\right)\prod_{k=1}^{n}\mathcal{N}\left(\mu^k_{t+1} + \sum_{X^j_t\in Pa(s^k_{t+1})}\beta^{jk}_{t}(X^j_t - \mu^j_t),(v^k_{t+1})^2\right)\right],
    \end{equation*}
    $$
    
&lt;p&gt;For the parameters $\pmb\mu^a, \pmb \mu^s,\pmb\sigma^2 \pmb v^2, \pmb \beta$, we have the &lt;strong&gt;conjugate prior&lt;/strong&gt;

    $$
    \begin{equation*}\small
    p(\pmb \mu^a, \pmb \mu^s,\pmb\sigma^2 \pmb v^2, \pmb \beta) =
    \prod_{t=1}^H\left(\prod_{k=1}^{m} p(\lambda^k_t)p\left((\sigma_k^{t})^2\right)\prod_{k=1}^{n} p(\mu^k_t)p\left((v^k_{t})^2\right) \prod_{i\neq j} p(\beta^{ij}_t)\right),
    \end{equation*}
    $$
    
where

    $$
    \begin{equation}
    \small
    \begin{split}
    &amp;p(\lambda^k_t) =\mathcal{N}\left({\lambda}^{k(0)}_t, \left({\delta}_{t,k}^{(\lambda)}\right)^2\right), \quad p(\mu^k_t) = \mathcal{N}\left({\mu}^{k(0)}_t, \left({\delta}_{t,k}^{(\mu)}\right)^2\right), \\
    &amp; p(\beta_{t}^{ij}) = \mathcal{N}\left({\beta}_{t}^{ij(0)}, \left({\delta}_{t,ij}^{(\beta)}\right)^2\right)\quad
     p\left((\sigma_t^k)^2\right) = \text{Inv-}\Gamma\left(\dfrac{\kappa_{t,k}^{(\sigma)}}{2}, \dfrac{\rho_{t,k}^{(\sigma)}}{2}\right), \\ 
    &amp;p((v_t^k)^2) = \text{Inv-}\Gamma\left(\dfrac{\kappa_{t,k}^{(v)}}{2}, \dfrac{\rho_{t,k}^{(v)}}{2}\right),
    \end{split}
    \end{equation}
    $$
    
where $\text{Inv-}\Gamma$ denotes the inverse-gamma distribution. We omit the tedious derivation for the posterior conditional distribution for each model parameters but you can find them in &lt;strong&gt;Appendix Section 10 of [&lt;a href=&#34;https://arxiv.org/pdf/2105.06543.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;4&lt;/a&gt;].&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Frequentists’ Approach&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Following [5] (p.318, Chapter 10), we can write the LG-DBN Eq.\eqref{eq: 5} as&lt;/p&gt;
&lt;p&gt;
$$
\begin{equation}\pmb\tau-\pmb\mu=B(\pmb\tau-\pmb\mu) - \Sigma^{1/2} \pmb u \tag{6}\label{eq: 6}\end{equation}
$$

where $\pmb{\mu} = [\pmb{\mu}^s_1,\pmb{\mu}^a_1,\ldots,\pmb{\mu}^s_{H},\pmb{\mu}^a_{H},\pmb{\mu}^s_{H+1}]^\top$, $\pmb u$ is an $\left((H+1)n+Hm\right)$-dimensional standard normal random vector, $\Sigma^{\frac{1}{2}}=\text{diag}\left(\pmb{v}^s_1,\pmb\sigma_1,\ldots,\pmb{v}^s_H,\pmb\sigma_H,\pmb{v}^s_{H+1}\right)$ is the diagonal matrix of the conditional standard deviations of state and actions, and the coefficient matrix of observed trajectory is written as

$$
B = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\pmb{\beta}^s_1 &amp; \pmb{\beta}^a_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0&amp; \pmb{\beta}^s_2 &amp; \pmb{\beta}^a_2 &amp;  0 &amp; 0 &amp;\cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \pmb{\beta}^s_H &amp; \pmb{\beta}^a_H &amp; 0 &amp; 0\\\end{bmatrix}
$$
&lt;/p&gt;
&lt;p&gt;Thus, by rearranging Eq.\eqref{eq: 6} and letting $\pmb{\tau}-\pmb\mu=(I-B)^{-1}\Sigma^{\frac{1}{2}} \pmb{u}$,  we have

$$
\pmb{\tau}\sim \mathcal{N}\left(\pmb\mu, (I-B)^{-1}\Sigma (I-B)^{-\top}\right)
$$

with mean $\mathbb{E}[\pmb\tau]=\pmb{\mu}$ and covariance matrix $\text{Cov}\left(\pmb{\tau}-\pmb\mu\right)=(I-B)^{-1}\Sigma(I-B)^{-\top}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimating $\pmb\mu$:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\tilde{\pmb\tau}
\equiv (\tilde{\pmb{s}}_1,\tilde{\pmb{a}}_1,\ldots,\tilde{\pmb{s}}_H,\tilde{\pmb{a}}_H,
\tilde{\pmb{s}}_{H+1})
=\pmb{\tau} - \pmb\mu$, where $\tilde{\pmb{s}}_t$ and $\tilde{\pmb{a}}_t$ denote centered observable state and decision. The unbiased estimator $\hat{\pmb\mu}=\frac{1}{N}\sum^N_{i=1}\pmb\tau^{(i)}$ can be easily obtained by using the fact $\mathbb{E}[\pmb\tau]=\pmb\mu$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimating $\pmb\beta_t^s,\pmb\beta_t^a,\Sigma$&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;The log-likelihood of the centered trajectory observations $\{\tilde{\pmb{\tau}}^{(i)}\}_{i=1}^N$ is,

$$
\begin{equation*}\scriptsize
\begin{split}
\max_{\pmb{{\beta}}^s, \pmb{{\beta}}^a,V} &amp; \ell\left(\tilde{\pmb\tau}^{(1)},\ldots, \tilde{\pmb\tau}^{(N)}; \pmb{{\beta}}^s, \pmb{{\beta}}^a,V \right) = \max_{\pmb{{\beta}}^s, \pmb{{\beta}}^a,V}\log\prod_{i=1}^N p\left(\tilde{\pmb\tau}^{(i)}\right) \\ &amp;=\max_{V_1} \sum_{i=1}^N\log p(\tilde{\pmb{s}}_1^{(i)}) \left[\sum_{t=1}^H\max_{\sigma_t}\sum_{i=1}^N \log p(\tilde{\pmb{a}}_t^{(i)}) \right] \left[\sum_{t=1}^H\max_{\pmb{{\beta}}_t^s,\pmb{{\beta}}_t^a,\pmb{v}_{t+1}} \sum_{i=1}^N\log p(\tilde{\pmb{s}}_{t+1}^{(i)}|\tilde{\pmb{s}}_t^{(i)},\tilde{\pmb{a}}_t^{(i)})\right]
\end{split}
\end{equation*}

$$

Since both initial state $\tilde{\pmb{s}}_1$ and actions $\tilde{\pmb{a}}_t$ are normally distributed with mean zero, the MLEs of their variance are just sample covariances:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{v}^{k}_1=\frac{1}{N}\sum^N_{i=1}\left(\tilde{{s}}_1^{k{(i)}}\right)^2$ with $k=1,2,\ldots,n$&lt;/li&gt;
&lt;li&gt;$\hat{\sigma}_t^k=\frac{1}{N}\sum^N_{i=1}\left(\tilde{{a}}_t^{k{(i)}}\right)^2$ with $k=1,2,\ldots,m$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, at any time $t$,  we have the log likelihood of a sample $\tilde{\pmb\tau}^{(i)}$

$$
\begin{equation*}\scriptsize
\log p(\tilde{\pmb{s}}_{t+1}^{(i)}|\tilde{\pmb{s}}_t^{(i)},\tilde{\pmb{a}}_t^{(i)}) \propto \frac{N}{2} \log|V_{t+1}|-\frac{1}{2} \left(\tilde{\pmb{s}}^{(i)}_{t+1}-\pmb{{\beta}}_t^s\tilde{\pmb{s}}^{(i)}_{t} -\pmb{{\beta}}_t^a\tilde{\pmb{a}}^{(i)}_{t}\right)^\top V_{t+1}\left(\tilde{\pmb{s}}^{(i)}_{t+1}-\pmb{{\beta}}_t^s\tilde{\pmb{s}}^{(i)}_{t} -\pmb{{\beta}}_t^a\tilde{\pmb{a}}^{(i)}_{t}\right)
\end{equation*}
$$

Let $\tilde{\pmb{s}}_{t+1}^{(i)}$ and $(\tilde{\pmb{s}}_{t}^{(i)}, \tilde{\pmb{a}}_{t}^{(i)})$ denote the $i$-th rows of output matrix $Y$ and the input matrix $X$, respectively.&lt;/p&gt;
&lt;p&gt;Let $B_{t}= \left(\pmb{{\beta}}^s_t,\pmb{{\beta}}^a_t\right)^\top$ denote the coefficient vector at time $t$.&lt;/p&gt;
&lt;p&gt;As a result, the maximum likelihood estimators (MLEs) of $\pmb{{\beta}}_t^s$ and $\pmb{{\beta}}_t^a$ are

$$
\begin{equation}\scriptsize
\left(\hat{\pmb\beta}_t^s,\hat{\pmb\beta}_t^a\right)^\top= \hat{B}_{t} = \arg\max_{B_{t}}-\frac{1}{2} \left(Y-XB_{t}\right)^\top (V_{t+1})^{-1}\left(Y-X B_{t}\right) = (X^\top (V_{t+1})^{-1}X)^{-1}X^\top (V_{t+1})^{-1} Y.  \tag{7}\label{eq: 7} \end{equation}
$$
&lt;/p&gt;
&lt;p&gt;The MLE of each standard deviation can be computed by $\hat{v}^{k}_t=\sqrt{\frac{1}{N}\sum^N{i=1}\left(\tilde{s}_t^{k{(i)}}\right)^2}$. In summary, from observations $\mathcal{D}$, the auxiliary MLE can then be obtained as $\hat{\pmb{\beta}} = (\hat{\pmb{\mu}}^s,\hat{\pmb\mu}^a,\hat{\pmb{\beta}}^s,\hat{\pmb{\beta}}^a,\hat{\pmb\sigma},\hat{\pmb{v}})$.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A simple explanation of the Frequentists’ approach is that the estimation of network can be viewed as solving initial node (initial state and action) separately and then local linear regression \eqref{eq: 7} for each time step.&lt;/li&gt;
&lt;li&gt;Thus, in practice, parameter estimation of LG-DBN is extremely simple: center the data and then apply a set of  linear regressions to solve Eq.\eqref{eq: 5}.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;nonlinear-dynamic-bayesian-network-based-hybrid-model-6httpsarxivorgpdf220103116pdf&#34;&gt;Nonlinear Dynamic Bayesian Network Based Hybrid Model [&lt;a href=&#34;https://arxiv.org/pdf/2201.03116.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;6&lt;/a&gt;]&lt;/h2&gt;
&lt;p&gt;Given the existing nonlinear ODE-based mechanistic model with the parameter $\pmb\beta$, represented by

$$
\begin{equation}
\text{d}\pmb{s}/\text{d}t = \pmb{f}\left(\pmb{s},\pmb{a}; \pmb\beta\right)
\end{equation}
$$
&lt;/p&gt;
&lt;p&gt;by using the finite difference approximations for derivatives, i.e., $\text{d} \pmb{s}\approx \Delta \pmb{s}_t=\pmb{s}_{t+1}-\pmb{s}_t$,
and $\text{d}t\approx \Delta t$, we construct the hybrid model for state transition,&lt;/p&gt;

$$
\begin{equation}
\pmb{s}_{t+1} = \pmb{s}_t + \Delta t \cdot \pmb{f}(\pmb{s}_t,\pmb{a}_t; \pmb{\beta}_t) + \pmb{e}^s_{t+1}\tag{8}\label{eq: 8}
\end{equation}
$$

&lt;p&gt;with unknown kinetic coefficients $\pmb{\beta}_t\in \mathbb{R}^{d_\beta}$.  The residual terms are modeled by  multivariate Gaussian distributions $\pmb{e}_{t+1}^{s} \sim \mathcal{N}(0,V_{t+1})$ with zero means and diagonal covariance matrices $V_{t+1}$.&lt;/p&gt;
&lt;p&gt;Let $g(\pmb{s}_t,\pmb{a}_t;\pmb{\beta}_t) \equiv \pmb{s}_t + \Delta t \cdot \pmb{f}(\pmb{s}_t,\pmb{a}_t; \pmb{\beta}_t)$. Then, at any time step $t$, we have the full state transition from $(\pmb s_t,\pmb a_t)$ to next state $\pmb s_{t+1}$&lt;/p&gt;
&lt;p&gt;
$$
\begin{equation}
\pmb{s}_{t+1}|\pmb{s}_{t},\pmb{a}_t=g(\pmb{s}_t,\pmb{a}_t;\pmb{\beta}_t) + \pmb{e}_{t+1} \sim \mathcal{N}\Big(g(\pmb{s}_t,\pmb{a}_t;\pmb{\beta}_t),  V_{t+1} \Big)\tag{9}\label{eq: 9}
\end{equation}
$$

where $V_{t+1}$is diagonal covariance matrix.&lt;/p&gt;
&lt;p&gt;In addition, if we define the initial conditions $\pmb{s}_1\sim \mathcal{N}(\pmb\mu_1,V_1)$ and assume the actions $\pmb a_t\sim \mathcal{N}(\lambda_t,\Sigma_t)$, the final model will have parameter $\pmb\theta_t=\{\mu_1,V_1,\lambda_t, \Sigma_t,\pmb\beta_t,V_{t+1}\}_{t=1}^H$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The reason we call Eq. \eqref{eq: 5},\eqref{eq: 9} hybrid models is that they can be derived from mechanistic model.&lt;/li&gt;
&lt;li&gt;the nonlinear DBN hybrid model can be also visualized by a &lt;strong&gt;directed acyclic network.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;In general, we can specify the initial state in any forms (e.g. fixed value, Beta random vector) and use a control strategy or policy to determine action, i.e. random policy $\pmb{a}_t\sim \pi(\pmb{a}_t|\pmb{s}_t)$ or  deterministic policy $\pmb{a}_t=\pi(\pmb{s}_t)$. We will discuss how to combine the reinforcement learning based control with this hybrid model in next blogs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parametric-variation-modeling&#34;&gt;Parametric Variation Modeling&lt;/h3&gt;
&lt;p&gt;To further account for sample-to-sample variation, the model coefficients $\pmb\beta$ are modeled by random variables

$$
\pmb\beta_t=\pmb\mu_t^\beta+\pmb\epsilon_t^\beta
$$

where $\pmb\epsilon^\beta_t$ represents the random effect following a multivariate normal distribution with mean zero and covariance matrix $\Sigma^\beta$. The vector $\pmb\mu_t^\beta$ represents the fixed effect of coefficients. Then the state transition becomes

$$
\begin{equation}
\pmb{s}_{t+1} =\pmb{g}(\pmb{s}_t,  \pmb{a}_t, \pmb{\beta}_t; \pmb{\theta}_t) + \pmb{e}^{s}_{t+1}\tag{10}\label{eq: 10}
\end{equation}
$$

where $\pmb\beta_t\sim \mathcal{N}(\pmb\mu_t^\beta,\Sigma^\beta_t)$ and $\pmb{e}^{s}_{t+1}\sim \mathcal{N}(0,V_{t+1})$. Therefore, the hybrid model is specified by the parameters $\pmb{\theta}_t=\left(\pmb\mu_t^\beta, \Sigma_t^\beta,V^x_{t+1},V^z_{t+1}\right)^\top$.&lt;/p&gt;
&lt;h3 id=&#34;latent-state-modeling&#34;&gt;Latent State Modeling&lt;/h3&gt;
&lt;p&gt;In many complex systems, there often exist unobservable state variables (such as cell growth inhibitor in fermentation system, culture in economic system, single-species behavior in ecosystem), which have substantial impact on model performance. In this section, we will show how to allow DBN based hybrid mode to incorporate the latent state.&lt;/p&gt;
&lt;p&gt;Let $\pmb{z}_t$ denote the latent state variable(s). At any time step $t$, the process state $\pmb{s}_t$ includes observable and unobservable (latent) state variables, i.e., $\pmb{s}_t=(\pmb{x}_t,\pmb{z}_t)$. Then Eq.\eqref{eq: 8} becomes

$$
\begin{align}
\pmb{x}_{t+1} &amp;= \pmb{x}_t + \Delta t \cdot \pmb{f}_x(\pmb{x}_t,\pmb{z}_t,\pmb{a}_t; \pmb{\beta}_t) + \pmb{e}^{x}_{t+1} \\
\pmb{z}_{t+1} &amp;= \pmb{z}_t + \Delta t \cdot \pmb{f}_z(\pmb{x}_t,\pmb{z}_t,\pmb{a}_t; \pmb{\beta}_t) + \pmb{e}^z_{t+1}
\end{align}
$$

with unknown $d_\beta$-dimensional kinetic coefficients $\pmb{\beta}_t\in \mathbb{R}^{d_\beta}$ (e.g., cell growth and inhibition rates). The function structures of $\pmb{f}_x(\cdot)$ and $\pmb{f}_z(\cdot)$ are derived from $\pmb{f}(\cdot)$ in the mechanistic models. The residual terms are modeled by  multivariate Gaussian distributions $\pmb{e}_{t+1}^{x} \sim \mathcal{N}(0,V^{x}_{t+1})$ and $\pmb{e}_{t+1}^{z} \sim \mathcal{N}(0,V^{z}_{t+1})$ with zero means and covariance matrices $V^{x}_{t+1}$ and $V^{z}_{t+1}$.&lt;/p&gt;
&lt;p&gt;The resuting model if often referred as to the &lt;strong&gt;probabilistic knowledge graph (KG) hybrid model&lt;/strong&gt; or &lt;strong&gt;dynamic Bayesian newtork (DBN) hybrid model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;They can be visualized by a directed network as shown in the figure below.&lt;/p&gt;
&lt;!-- ![BN.png](BN.png) --&gt;
















&lt;figure  id=&#34;figure-probabilistic-knowledge-graph-hybrid-model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Probabilistic Knowledge Graph Hybrid Model.&#34; srcset=&#34;
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/BN_hu63dbe40e14337ab41c34947728e91f33_44244_80f56804cf82d50bf929758e16cc89a0.webp 400w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/BN_hu63dbe40e14337ab41c34947728e91f33_44244_f31ca5f94516d804de20475510a6164b.webp 760w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/BN_hu63dbe40e14337ab41c34947728e91f33_44244_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zhenghuazx.github.io/hua.zheng/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/BN_hu63dbe40e14337ab41c34947728e91f33_44244_80f56804cf82d50bf929758e16cc89a0.webp&#34;
               width=&#34;703&#34;
               height=&#34;309&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Probabilistic Knowledge Graph Hybrid Model.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The observable state $\pmb{x}_t$ and latent state $\pmb{z}_t$ are represented by solid and shaded nodes respectively. The directed edges represent causal interactions. At any time period $t+1$, the process state output node $\pmb{s}_{t+1}=(\pmb{x}_{t+1},\pmb{z}_{t+1})$ depends on its parent nodes: $\pmb{s}_{t+1}=\pmb{f}(Pa(\pmb{s}_{t+1});{\pmb\theta}_t)$ with parent nodes $Pa(\pmb{s}_{t+1})=(\pmb{s}_t,\pmb{a}_t,\pmb{e}_{t+1})$ and model parameter $\pmb{\theta}_t=(\pmb{\beta}_t,{V}_{t+1})$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Complete Hybrid Model&lt;/strong&gt;
Now let&amp;rsquo;s further incorporate the parametric variations and latent state into hybrid model. Eq.\eqref{eq: 10} becomes&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}    \pmb{x}_{t+1} &amp;=\pmb{g}_x(\pmb{x}_t, \pmb{z}_t, \pmb{a}_t, \pmb{\beta}_t; \pmb{\theta}_t) + \pmb{e}^{x}_{t+1} \\    \pmb{z}_{t+1} &amp;= \pmb{g}_z(\pmb{x}_t, \pmb{z}_t, \pmb{a}_t, \pmb{\beta}_t; \pmb{\theta}_t) + \pmb{e}^{z}_{t+1}    ,\end{align}
$$

where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\pmb{g}_x(\pmb{x}_t, \pmb{z}_t, \pmb{a}_t; \pmb{\beta}_t)\equiv\pmb{x}_t + \Delta t \cdot \pmb{f}_x(\pmb{x}_t,\pmb{z}_t,\pmb{a}_t; \pmb{\beta}_t)$&lt;/li&gt;
&lt;li&gt;$\pmb{g}_z(\pmb{x}_t, \pmb{z}_t, \pmb{a}_t; \pmb{\beta}_t)\equiv\pmb{z}_t + \Delta t \cdot \pmb{f}_z(\pmb{x}_t,\pmb{z}_t,\pmb{a}_t; \pmb{\beta}_t)$,&lt;/li&gt;
&lt;li&gt;$\pmb{e}^{x}_{t+1}\sim \mathcal{N}(0,V^x_{t+1})$, and $\pmb{e}^{z}_{t+1}\sim \mathcal{N}(0,V^z_{t+1})$,&lt;/li&gt;
&lt;li&gt;$\pmb\beta_t\sim \mathcal{N}(\pmb\mu_t^\beta,\Sigma^\beta_t)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inference-a-bayesian-approach&#34;&gt;Inference (A Bayesian Approach)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Observable Process Trajectory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By integrating out latent states $(\pmb{z}_1,\ldots,\pmb{z}_{H+1})$, we have the likelihood of the partially observed trajectory $\pmb\tau_x\equiv (\pmb{x}_1,\pmb{a}_1,\ldots,\pmb{x}_H,\pmb{a}_H,\pmb{x}_{H+1})$, i.e.,

$$
\begin{equation}
p(\pmb{\tau}_x| \pmb\theta)= \int p(\pmb\tau|\pmb\theta) d \pmb{z}_1 \cdots d \pmb{z}_{H+1}\tag{11}\label{eq: 11}
\end{equation}
$$

Then we can focus on estimating \pmb\theta from the likelihood \eqref{eq: 11} by using approximate Bayesian computation-sequential Monte Carlo (ABC-SMC) method.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ABC-SMC&lt;/strong&gt; sampling procedure for generating posterior samples from $p(\pmb{\theta}|\mathcal{D})$ (see [7-9]).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Main Idea:&lt;/strong&gt; Given the observable trajectory $\pmb\tau_x=(\pmb{x}_1,\pmb{a}_1,\ldots,\pmb{x}_H,\pmb{a}_H,\pmb{x}_{H+1})$.

$$
\begin{equation}
p(\pmb\theta|\pmb\tau_x) \propto p(\pmb\tau_x| \pmb\theta)p(\pmb\theta)
\end{equation}
$$

The algorithm samples $\pmb\theta$ and $\pmb\tau^\star_x$ from the joint posterior:

$$
\begin{equation}
p_\delta(\pmb\theta,\pmb\tau_x^\star| \pmb\tau_x)=\frac{p(\pmb\theta)p(\pmb\tau_x^\star|\pmb\theta)\mathbb{1}_\delta[\pmb\tau_x^\star]}{\int\int p(\pmb\theta)p(\pmb\tau^\star_x|\pmb\theta)\mathbb{1}_\delta[\pmb\tau_x^\star]d\pmb\tau_x^\star d\pmb\theta}
\end{equation}
$$

where $\mathbb{1}_\delta[\pmb\tau_x^\star] = \mathbb{1}_\delta[d(\pmb\tau_x,\pmb\tau_x^\star)\leq \delta]$ is one if $d(\pmb\tau_x,\pmb\tau_x^\star)\leq \delta$ and zero else. When $\delta$ is small, $p_{\delta}(\pmb\theta|\pmb\tau_x)=\int p_\delta(\pmb\theta,\pmb\tau_x^\star| \pmb\tau)d\pmb\tau_x^\star$ is a good approximation to $p(\pmb\theta | \pmb\tau_x)$ because&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the distance $d(\pmb\tau_x,\pmb\tau_x^\star)\leq \delta$ is small, $\mathbb{1}_\delta[\pmb\tau_x^\star]\rightarrow1$ and Eq.(17) becomes

    $$
    p_\delta(\pmb\theta,\pmb\tau_x^\star| \pmb\tau_x)\rightarrow\frac{p(\pmb\theta)p(\pmb\tau_x^\star|\pmb\theta)}{\int\int p(\pmb\theta)p(\pmb\tau^\star_x|\pmb\theta)d\pmb\tau_x^\star d\pmb\theta}=p(\pmb\theta,\pmb\tau_x^\star|\pmb\tau_x)
    $$
    &lt;/li&gt;
&lt;li&gt;Then $p_{\delta}(\pmb\theta|\pmb\tau_x)=\int p_\delta(\pmb\theta,\pmb\tau_x^\star| \pmb\tau)d\pmb\tau^\star_x\rightarrow\int p(\pmb\theta,\pmb\tau_x^\star|\pmb\tau_x)d\pmb\tau^\star_x=p(\pmb\theta|\pmb\tau_x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Low acceptation rate when using the full trajectory $\tau_x$ as summary statistics. It leads to low computational efficiency.&lt;/li&gt;
&lt;li&gt;Directly compute the distance between simulated and observed trajectories $d(\pmb\tau_x,\pmb\tau_x^\star)$ is computationally expensive and uninformative.Thus instead of directly using trajectory, we can consider sufficient summary statistics of trajectories $\eta(\pmb\tau_x)$ and use the distance measure $d\left(\eta(\tau_x),\eta(\tau^\star_x)\right)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;New Inference Method&lt;/strong&gt;: [10] propose a new auxiliary based ABC-SMC method. Check out [10] on arXiv.&lt;/p&gt;
&lt;p&gt;In short, we solved a problem with observable state ($\rho$) and latent variable ($I$) in the following form:

$$
\begin{align}    \rho_{t+1} &amp;= \rho_t + \Delta t \cdot r_g \rho_t \Bigg (1 - \Big(1+e^{(k_s(k_c-I_t))} \Big) ^{-1} \Bigg ) + e^{\rho}_{t}\\   I_{t+1} &amp;= I_t + \Delta t \cdot \Bigg (\frac{\rho_{t+1}-\rho_t}{\Delta t} - r_d I_t \Bigg) + e^I_t \end{align}
$$
&lt;/p&gt;
&lt;p&gt;We compared two different approximate Bayesian computation method to estimate the posterior distribution $p(\pmb\theta|\mathcal{D})$:  LG-DBN auxiliary likelihood-based ABC-SMC approach and naive ABC-SMC.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 4(a): Posterior predictive distributionn of inhibitor with process noise v=0.1, 0.2 and sample size m=3,6,20. (LG-DBN auxiliary based ABC-SMC)&#34; srcset=&#34;
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/ABC-SMC_hu9118d3ccac408e7a3f2d5f15e5e49428_836324_37151a4275ec16f38ca8b32862e3d5ca.webp 400w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/ABC-SMC_hu9118d3ccac408e7a3f2d5f15e5e49428_836324_d558239f790e8ce888940b62ce103f07.webp 760w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/ABC-SMC_hu9118d3ccac408e7a3f2d5f15e5e49428_836324_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zhenghuazx.github.io/hua.zheng/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/ABC-SMC_hu9118d3ccac408e7a3f2d5f15e5e49428_836324_37151a4275ec16f38ca8b32862e3d5ca.webp&#34;
               width=&#34;653&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;!-- 
















&lt;figure  id=&#34;figure-posterior-distributions-of-r_g-6-macro-replications-the-posterior-distributions-estimated-by-lg-dbn-auxiliary-based-abc-smc--the-black-dashed-lines-represent-the-true-value-of-parameters&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Posterior distributions of $r_g$ (6 macro replications). The posterior distributions estimated by LG-DBN auxiliary based ABC-SMC . The black dashed lines represent the “true” value of parameters.&#34; srcset=&#34;
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/r_g_with_auxilary_hu849c0ab1aa055d985bcd607a5c33e827_41300_4dfdde34e24249c7f6e088dcfad4ced4.webp 400w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/r_g_with_auxilary_hu849c0ab1aa055d985bcd607a5c33e827_41300_dacf63569a2bbc7e0f51e68bae5472e4.webp 760w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/r_g_with_auxilary_hu849c0ab1aa055d985bcd607a5c33e827_41300_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zhenghuazx.github.io/hua.zheng/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/r_g_with_auxilary_hu849c0ab1aa055d985bcd607a5c33e827_41300_4dfdde34e24249c7f6e088dcfad4ced4.webp&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Posterior distributions of $r_g$ (6 macro replications). The posterior distributions estimated by LG-DBN auxiliary based ABC-SMC . The black dashed lines represent the “true” value of parameters.
    &lt;/figcaption&gt;&lt;/figure&gt;


















&lt;figure  id=&#34;figure-posterior-distributions-of-r_g6-macro-replications-the-posterior-distributions-estimated-by-naive-abc-smc-the-black-dashed-lines-represent-the-true-value-of-parameters&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Posterior distributions of $r_g$(6 macro replications). The posterior distributions estimated by naive ABC-SMC. The black dashed lines represent the “true” value of parameters.&#34; srcset=&#34;
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/r_g_without_auxilary_hu849c0ab1aa055d985bcd607a5c33e827_64017_7632eb10dca4246d6ab81244f893d15d.webp 400w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/r_g_without_auxilary_hu849c0ab1aa055d985bcd607a5c33e827_64017_f8540c69875f415ff49279bb14096a90.webp 760w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/r_g_without_auxilary_hu849c0ab1aa055d985bcd607a5c33e827_64017_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zhenghuazx.github.io/hua.zheng/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/r_g_without_auxilary_hu849c0ab1aa055d985bcd607a5c33e827_64017_7632eb10dca4246d6ab81244f893d15d.webp&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Posterior distributions of $r_g$(6 macro replications). The posterior distributions estimated by naive ABC-SMC. The black dashed lines represent the “true” value of parameters.
    &lt;/figcaption&gt;&lt;/figure&gt;


















&lt;figure  id=&#34;figure-posterior-predictive-distribution-of-cell-density-with-process-noise-v01-02-and-sample-size-m3620-lg-dbn-auxiliary-based-abc-smc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Posterior predictive distribution of cell density with process noise v=0.1, 0.2 and sample size m=3,6,20 (LG-DBN auxiliary based ABC-SMC)&#34; srcset=&#34;
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/cell_density_with_auxilary_hu116ab78b5d377eadb056e2ab3531dae6_73930_1d9a71411f85de261f4b3f66931d9283.webp 400w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/cell_density_with_auxilary_hu116ab78b5d377eadb056e2ab3531dae6_73930_08ddfb4d63a6edb8e60a84d7e71a9edf.webp 760w,
               /hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/cell_density_with_auxilary_hu116ab78b5d377eadb056e2ab3531dae6_73930_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zhenghuazx.github.io/hua.zheng/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/cell_density_with_auxilary_hu116ab78b5d377eadb056e2ab3531dae6_73930_1d9a71411f85de261f4b3f66931d9283.webp&#34;
               width=&#34;760&#34;
               height=&#34;456&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Posterior predictive distribution of cell density with process noise v=0.1, 0.2 and sample size m=3,6,20 (LG-DBN auxiliary based ABC-SMC)
    &lt;/figcaption&gt;&lt;/figure&gt;



![Figure 4(a): Posterior predictive distributionn of inhibitor with process noise v=0.1, 0.2 and sample size m=3,6,20. (LG-DBN auxiliary based ABC-SMC)](I_with_auxilary.png)

Figure 4(a): Posterior predictive distributionn of inhibitor with process noise v=0.1, 0.2 and sample size m=3,6,20. (LG-DBN auxiliary based ABC-SMC)

![Figure 3(b): Posterior predictive distribution of cell density with process noise v=0.1, 0.2 and sample size m=3,6,20. (naive ABC-SMC)](cell_density_without_auxilary.png)

Figure 3(b): Posterior predictive distribution of cell density with process noise v=0.1, 0.2 and sample size m=3,6,20. (naive ABC-SMC)

![Figure 4(b): Posterior predictive distributionn of inhibitor with process noise v=0.1, 0.2 and sample size m=3,6,20. (naive ABC-SMC)](I_without_auxilary.png)

Figure 4(b): Posterior predictive distributionn of inhibitor with process noise v=0.1, 0.2 and sample size m=3,6,20. (naive ABC-SMC) --&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I briefly reviewed the theoretical basics of dynamic Bayesian network based hybrid model with a short discussion about its inference. I hope this new model class could inspire other researchers to model complex stochastic process in different domain.&lt;/p&gt;
&lt;h1 id=&#34;next&#34;&gt;Next?&lt;/h1&gt;
&lt;p&gt;In the next blogs, I will discuss more on (1) the parameter estimation, (2) generalize the model to policy augmented dynamic Bayesian newtork (PADBN) by incoporating reinforcement learning, (3) may provide more interpretability tools such as Shaley value based factor importance (closed form); (4) more about the implementation.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Mockus, L., Peterson, J. J., Lainez, J. M., &amp;amp; Reklaitis, G. V. (2015). Batch-to-batch variation: a key component for modeling chemical manufacturing processes. &lt;em&gt;Organic Process Research &amp;amp; Development&lt;/em&gt;, &lt;em&gt;19&lt;/em&gt;(8), 908-914.&lt;/li&gt;
&lt;li&gt;Vasdekis, A. E., Silverman, A. M., &amp;amp; Stephanopoulos, G. (2015). Origins of cell-to-cell bioprocessing diversity and implications of the extracellular environment revealed at the single-cell level. &lt;em&gt;Scientific Reports&lt;/em&gt;, &lt;em&gt;5&lt;/em&gt;(1), 1-7.&lt;/li&gt;
&lt;li&gt;Dickens, J., Khattak, S., Matthews, T. E., Kolwyck, D., &amp;amp; Wiltberger, K. (2018). Biopharmaceutical raw material variation and control. &lt;em&gt;Current opinion in chemical engineering&lt;/em&gt;, &lt;em&gt;22&lt;/em&gt;, 236-243.&lt;/li&gt;
&lt;li&gt;Zheng, H., Xie, W., Ryzhov, I. O., &amp;amp; Xie, D. (2021). Policy optimization in bayesian network hybrid models of biomanufacturing processes. &lt;em&gt;arXiv preprint arXiv:2105.06543&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Murphy, K. P. (2012). &lt;em&gt;Machine learning: a probabilistic perspective&lt;/em&gt;. MIT press.&lt;/li&gt;
&lt;li&gt;Zheng, H., Xie, W., Wang, K., &amp;amp; Li, Z. (2022). Opportunities of Hybrid Model-based Reinforcement Learning for Cell Therapy Manufacturing Process Development and Control. &lt;em&gt;arXiv preprint arXiv:2201.03116&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Toni, T., Welch, D., Strelkowa, N., Ipsen, A., &amp;amp; Stumpf, M. P. (2009). Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems. &lt;em&gt;Journal of the Royal Society Interface&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(31), 187-202.&lt;/li&gt;
&lt;li&gt;Lenormand, M., Jabot, F., &amp;amp; Deffuant, G. (2013). Adaptive approximate Bayesian computation for complex models. &lt;em&gt;Computational Statistics&lt;/em&gt;, &lt;em&gt;28&lt;/em&gt;(6), 2777-2796.&lt;/li&gt;
&lt;li&gt;Del Moral, P., Doucet, A., &amp;amp; Jasra, A. (2006). Sequential monte carlo samplers. &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt;, &lt;em&gt;68&lt;/em&gt;(3), 411-436.&lt;/li&gt;
&lt;li&gt;Xie, W., Wang K., Zheng H. &amp;amp; Feng B (2022)Dynamic Bayesian Network Auxiliary ABC-SMC for Hybrid Model Bayesian Inference to Accelerate Biomanufacturing Process Mechanism Learning and Robust Control. &lt;em&gt;arXiv preprint arXiv:2205.02410&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>32nd Annual POMS-Conference (Talk 2)</title>
      <link>https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-2/</link>
      <pubDate>Mon, 25 Apr 2022 14:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Virtual Laboratory</title>
      <link>https://zhenghuazx.github.io/hua.zheng/project/vlab/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/project/vlab/</guid>
      <description>&lt;p&gt;vLab is a package for biopharmaceutical manufacturing simulation in Python. It utilizes the &lt;code&gt;diffeqpy&lt;/code&gt;,
a python wrapper of &lt;a href=&#34;http://diffeq.sciml.ai/dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DifferentialEquations.jl&lt;/a&gt; and scipy built-in ode solver
for its core routines to give high performance solving of large stiff differential equations, currently
including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N-linked Gycosylation Simulation in Perfusion Bioreactor&lt;/li&gt;
&lt;li&gt;Raman Spectrometry and Raman Data Analytics&lt;/li&gt;
&lt;li&gt;Plantwise Simulation (Cell Culture and Chromatography)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /hua.zheng/project/vlab/simulation_hua58c4e7b2b6fe9cda0c02501bb32db1c_885464_a0f66eab6cc7499f24c236af3a3e2b70.webp 400w,
               /hua.zheng/project/vlab/simulation_hua58c4e7b2b6fe9cda0c02501bb32db1c_885464_3bd80d72ab635699bceb8bea3ef685fc.webp 760w,
               /hua.zheng/project/vlab/simulation_hua58c4e7b2b6fe9cda0c02501bb32db1c_885464_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zhenghuazx.github.io/hua.zheng/hua.zheng/project/vlab/simulation_hua58c4e7b2b6fe9cda0c02501bb32db1c_885464_a0f66eab6cc7499f24c236af3a3e2b70.webp&#34;
               width=&#34;760&#34;
               height=&#34;448&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The end goal of this project is to build a modularized platform, composed of an interactive interface and a digital twin based “vLab”. The digital twin is designed to simulate end-to-end bioprocess by using state-of-the-art mechanistic models for prediction, control and risk management.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>32nd Annual POMS-Conference (Talk 1)</title>
      <link>https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-1/</link>
      <pubDate>Sat, 23 Apr 2022 16:30:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic Bayesian Network Auxiliary ABC-SMC for Hybrid Model Bayesian Inference to Accelerate Biomanufacturing Process Mechanism Learning and Robust Control</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/xie-2022-dynamic/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/xie-2022-dynamic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Opportunities of Hybrid Model-based Reinforcement Learning for Cell Therapy Manufacturing Process Development and Control</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-opportunities/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-opportunities/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Variance Reduction based Partial Trajectory Reuse to Accelerate Policy Gradient Optimization</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-variance/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-variance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Global-Local Metamodel-Assisted Stochastic Programming via Simulation</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/10-1145-3411080/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/10-1145-3411080/</guid>
      <description></description>
    </item>
    
    <item>
      <title>INFORMS Annual 2021</title>
      <link>https://zhenghuazx.github.io/hua.zheng/talk/informs-annual-2021/</link>
      <pubDate>Tue, 26 Oct 2021 09:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/talk/informs-annual-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>STEM Education and Industry Workforce Life-Long Training Platform Development to Faciliate Smart Biopharmaceutical Manufacturing 4.0-4</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/xie-2020-stem/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/xie-2020-stem/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Personalized Multimorbidity Management for Patients with Type 2 Diabetes Using Reinforcement Learning of Electronic Health Records</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021/</guid>
      <description>&lt;h2 id=&#34;prescription-by-ai-doctor&#34;&gt;Prescription by AI Doctor&lt;/h2&gt;
&lt;p&gt;We believe the potential for the use of AI is dramatic especially as it can assist family clinicians who are usually overloaded with patients to make better choices for treatment of their type 2 patients in order to help prevent hyperglycemia, hypertension and CV risk outcomes.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/DJBN7dGUvWg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EHR-RL</title>
      <link>https://zhenghuazx.github.io/hua.zheng/project/ehr-rl/</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/project/ehr-rl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reinforcement Learning Assisted Oxygen Therapy for COVID-19 Patients Under Intensive Care</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-reinforcement/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-reinforcement/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Winter Simulation Conference 2020</title>
      <link>https://zhenghuazx.github.io/hua.zheng/talk/winter-simulation-conference-2020/</link>
      <pubDate>Tue, 15 Dec 2020 10:30:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/talk/winter-simulation-conference-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Green Simulation Assisted Reinforcement Learning with Model Risk for Biomanufacturing Learning and Control</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/10-5555-3466184-3466221/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/10-5555-3466184-3466221/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Toxic Comment Classification</title>
      <link>https://zhenghuazx.github.io/hua.zheng/project/toxiccomment/</link>
      <pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/project/toxiccomment/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://zhenghuazx.github.io/hua.zheng/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
