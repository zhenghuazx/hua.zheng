<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning | Zheng.H</title>
    <link>https://zhenghuazx.github.io/hua.zheng/tag/reinforcement-learning/</link>
      <atom:link href="https://zhenghuazx.github.io/hua.zheng/tag/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Reinforcement Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Hua Zheng ©2022</copyright><lastBuildDate>Tue, 11 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://zhenghuazx.github.io/hua.zheng/media/icon_hu658dc7ef748f26cdf7d87fd659137d7d_44050_512x512_fill_lanczos_center_3.png</url>
      <title>Reinforcement Learning</title>
      <link>https://zhenghuazx.github.io/hua.zheng/tag/reinforcement-learning/</link>
    </image>
    
    <item>
      <title>Policy Optimization in Bayesian Network Hybrid Models of Biomanufacturing Processes</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-policy/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-policy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Variance Reduction based Experience Replay for Policy Optimization</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-green/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-green/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Experience replay allows agents to remember and reuse historical transitions. However, the uniform reuse strategy regardless of their significance is implicitly biased toward out-of-date observations. To overcome this limitation, we propose a general variance reduction based experience reply (VRER) approach, which allows policy optimization algorithms to selectively reuse the most relevant samples and improve policy gradient estimation. It tends to put more weight on historical observations that are more likely sampled from the target distribution. Different from other ER methods VRER is a theoretically justified and simple-to-use approach. Our theoretical and empirical studies demonstrate that the proposed VRER can accelerate the learning of optimal policy and enhance the performance of state-of-the-art policy optimization approaches.Experience replay allows agents to remember and reuse historical transitions. However, the uniform reuse strategy regardless of their significance is implicitly biased toward out-of-date observations. To overcome this limitation, we propose a general variance reduction based experience reply (VRER) approach, which allows policy optimization algorithms to selectively reuse the most relevant samples and improve policy gradient estimation. It tends to put more weight on historical observations that are more likely sampled from the target distribution. Different from other ER methods VRER is a theoretically justified and simple-to-use approach. Our theoretical and empirical studies demonstrate that the proposed VRER can accelerate the learning of optimal policy and enhance the performance of state-of-the-art policy optimization approaches.&lt;/p&gt;
&lt;h2 id=&#34;open-source-library-vrer-pg&#34;&gt;Open-source Library: vrer-pg&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;vrer-pg&lt;/strong&gt; is a tensorflow based AI library which facilitates experimentation with existing policy optimization algorithms with variance reduction based experience replay. It provides well tested components that can be easily modified or extended. The available selection of algorithms can be used directly or through command line.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;feature.jpg&#34;  /&gt;
  &lt;figcaption&gt;
      &lt;h4&gt;vrer-pg&lt;/h4&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The performance improvement of state-of-the-art PO algorithms after using VRER. Results are described by the mean performance curves and 95% confidence intervals of PPO(-VRER), TRPO(-VRER) and VPG(-VRER).&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;convergenec-VPG.png&#34;  /&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;convergenec-PPO.png&#34;  /&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;convergenec-TRPO.png&#34;  /&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Some Gaps Between Reinforcement Learning Practice and Theory</title>
      <link>https://zhenghuazx.github.io/hua.zheng/post/somegapsrl/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/post/somegapsrl/</guid>
      <description>&lt;p&gt;I am recently encountering some theoretical problems during a reinforcement learning (RL) research project. I started to realize how much difference between how people use versus how people prove RL algorithms. Although rarely mentioned in the literature, some gaps indeed exist and may be useful to think a little deeper. It motivates me to write some of my thoughts down.&lt;/p&gt;
&lt;p&gt;Note: Some of the practical heuristics might already have theoretical justifications that I do not know or bypassed by some reasonable assumptions. Thus, please correct me if I misunderstand or have mistakes.&lt;/p&gt;
&lt;h1 id=&#34;experience-replay-cause-bias&#34;&gt;Experience Replay Cause Bias&lt;/h1&gt;
&lt;p&gt;Off-policy learning is a large category of RL algorithms. it, instead of following the target policy, enables a target policy to be learned by using data from behavior policies. Experience replay is central to off-policy algorithms in deep reinforcement learning (RL). As the fundamental data-generating mechanism in off-policy deep reinforcement learning, it has been shown to improve sample efficiency and stability by storing and randomly replaying a fixed number of the most recently collected transitions for training.&lt;/p&gt;
&lt;p&gt;One interesting problem of experience replay is that the stored transitions are inevitably dependent with the current/target policy or value function, because the policy/value function parameter is in fact constructed from those transitions. In other words, all transitions are correlated. It &lt;strong&gt;violates&lt;/strong&gt; the following conditional unbiasedness assumption in stochastic policy gradient methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The estimated gradient is unbiased condition on the history (filtration) $\mathcal{F}_k=\{\xi_1,\ldots,\xi_{k};\theta_0,\theta_1,\ldots,\theta_{k}\}$, such that $\mathbb{E}_\xi[g(\theta_k,\xi)|\mathcal{F}_{k}]=\nabla f(\theta_k)$. ($\mathcal{F}_{k}$ denotes the outcomes up to and including time $k$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the classical stochastic gradient descent (SGD), the data are assumed to be sampled from a unknown population, which is also referred as to the behavior distribution in off-policy batch RL. However, when you have a growing experience replay buffer in which transitions were continuously collected by following past policies, you won’t have a fixed unknown behavior distribution but a changing known one. The main concerns behind this conditional dependence makes theoretical study of experience replay difficult.&lt;/p&gt;
&lt;h1 id=&#34;compatibility-of-critic-model&#34;&gt;Compatibility of Critic Model&lt;/h1&gt;
&lt;p&gt;Compatibility is an important concept in actor-critic methods, which are a popular class of on-policy RL algorithms for computing an optimal policy. The actor-critic consists of two eponymous components. An actor adjusts the parameters of the stochastic policy by stochastic gradient ascent. A critic estimates the action-value function to evaluate the action chosen by current policy. Instead of the unknown true action-value function, an action-value function is used, which introduce the concept of compatibility.&lt;/p&gt;
&lt;p&gt;In general, substituting a function approximator for the true action-value (or value) function may introduce bias. However, the bias is zero if the function approximator is compatible, i.e.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the function approximator is linear in the score function of policy&lt;/li&gt;
&lt;li&gt;parameters of the function approximator are chosen to minimize the mean-squared error of approximated and true action-value function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In practice, condition 2 is usually relaxed in favor of policy evaluation algorithms that estimate the value function more efficiently by temporal-difference learning. However, condition 1 is rarely satisfied. This issues applies to almost all, if not all, modern deep policy optimization algorithms.&lt;/p&gt;
&lt;p&gt;Good news is that people has released this issue and some literature are working on relaxing the compatibility constraint for neural network [1-3].&lt;/p&gt;
&lt;h1 id=&#34;discounted-objective&#34;&gt;Discounted Objective&lt;/h1&gt;
&lt;p&gt;I had recently revisited the policy gradient theorem proof when working on my actor-critic related paper, which leads to this question.&lt;/p&gt;
&lt;p&gt;After a quick literature review I found Russo Alessio mentioned the same issue in his blog [4] and Nota and Thomas [5] had raised concerns about the similar issue by constructing a counterexample. In addition, Naik et al [6] criticize discounted objective as well but in a different context.
I personally used a lot of discounted objective, but still felt unclear on some concepts. Part of the reason comes from inaccurate or ambiguous description and the other part I think comes from the fact that RL is an interdisciplinary area including contribution from optimization, operations research, statistics and computer science.
As a result, people used different terminologies and notations, which further exaggerates the ambiguity.&lt;/p&gt;
&lt;p&gt;Now let’s briefly discuss the issue in infinite-horizon MDP setting. For detailed discussion, I recommend reading the blog and literature mentioned above.&lt;/p&gt;
&lt;p&gt;In the classic policy gradient theorem, the discounted policy gradient, tells us how to modify the policy parameters, $\theta$, in order to increase $\nabla J_\gamma(\theta)$ , and is given by:&lt;/p&gt;

$$
\begin{equation}
\begin{split}
\nabla J_\gamma(\theta)
&amp;= \mathbb{E}_{s_t\sim d^\pi_\gamma,a_t\sim \pi(s_t|a_t)}\left[\left. \nabla \log\pi_\theta(a_t|s_t)Q^\pi_\gamma(s_t,a_t)\right|\theta\right]
\end{split}
\end{equation}
$$

&lt;p&gt;where $d^\pi_\gamma(s)=\int p(s_0)\sum^\infty_{t=0}\gamma^t p^\pi(s_t=s|s_0)d s_0$ is the discounted stationary state distribution (discounted state occupation).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: the samples used to update the policy should be distributed according to the discounted state distribution $d^\pi_\gamma(s)$ however, the sample actually used in most algorithm are $s_{t+1}\sim p(s_{t+1}|s_t,a_t),a_t\sim \pi(s_t|a_t)$. In average return case, it is allowed because we can reach the limiting distribution by continuously running the simulation, i.e. $d^\pi(s)=\lim_{t\rightarrow \infty}p(s_{t}=s|s_0)$. But in the discounted case, we can&amp;rsquo;t naively take average of samples from reply buffer as the discounted state stationary distribution treat observations sampled at different time steps differently.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Wang, D., &amp;amp; Hu, M. (2021). Deep Deterministic Policy Gradient With Compatible Critic Network. &lt;em&gt;IEEE Transactions on Neural Networks and Learning Systems&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Diddigi, R. B., Jain, P., &amp;amp; Bhatnagar, S. (2021). Neural Network Compatible Off-Policy Natural Actor-Critic Algorithm. &lt;em&gt;arXiv preprint arXiv:2110.10017&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Balduzzi, D., &amp;amp; Ghifary, M. (2015). Compatible value gradients for reinforcement learning of continuous deep policies. &lt;em&gt;arXiv preprint arXiv:1509.03005&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Russo Alessio. (2021) Why there is a problem with the Policy Gradient theorem in Deep Reinforcement Learning. &lt;a href=&#34;https://towardsdatascience.com/why-there-is-a-problem-with-the-policy-gradient-theorem-in-deep-reinforcement-learning-958d845218f1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://towardsdatascience.com/why-there-is-a-problem-with-the-policy-gradient-theorem-in-deep-reinforcement-learning-958d845218f1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chris Nota and Philip S. Thomas. 2020. Is the Policy Gradient a Gradient?. AAMAS (2020).&lt;/li&gt;
&lt;li&gt;Naik, A., Shariff, R., Yasui, N., Yao, H., &amp;amp; Sutton, R. S. (2019). Discounted reinforcement learning is not an optimization problem. NeurIPS 2019.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>32nd Annual POMS-Conference (Talk 2)</title>
      <link>https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-2/</link>
      <pubDate>Mon, 25 Apr 2022 14:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Opportunities of Hybrid Model-based Reinforcement Learning for Cell Therapy Manufacturing Process Development and Control</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-opportunities/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-opportunities/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Personalized Multimorbidity Management for Patients with Type 2 Diabetes Using Reinforcement Learning of Electronic Health Records</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021/</guid>
      <description>&lt;h2 id=&#34;prescription-by-ai-doctor&#34;&gt;Prescription by AI Doctor&lt;/h2&gt;
&lt;p&gt;We believe the potential for the use of AI is dramatic especially as it can assist family clinicians who are usually overloaded with patients to make better choices for treatment of their type 2 patients in order to help prevent hyperglycemia, hypertension and CV risk outcomes.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/DJBN7dGUvWg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EHR-RL</title>
      <link>https://zhenghuazx.github.io/hua.zheng/project/ehr-rl/</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/project/ehr-rl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reinforcement Learning Assisted Oxygen Therapy for COVID-19 Patients Under Intensive Care</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-reinforcement/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-reinforcement/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Winter Simulation Conference 2020</title>
      <link>https://zhenghuazx.github.io/hua.zheng/talk/winter-simulation-conference-2020/</link>
      <pubDate>Tue, 15 Dec 2020 10:30:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/talk/winter-simulation-conference-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Green Simulation Assisted Reinforcement Learning with Model Risk for Biomanufacturing Learning and Control</title>
      <link>https://zhenghuazx.github.io/hua.zheng/publication/10-5555-3466184-3466221/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://zhenghuazx.github.io/hua.zheng/publication/10-5555-3466184-3466221/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
