[{"authors":null,"categories":null,"content":"Hua is a former ML Engineer and currently working on his PhD at Northeastern University. His industrial experience includes large-scale data engineering, big data, product search and recommender system. His research concentrates on (deep) reinforcement learning, stochastic optimization and (dynamic) Bayesian network. With years of research and industry experience, he has built a multi-disciplinary skill set across computer science, optimization, machine learning, and statistics.\nDownload my resumé.\n","date":1665446400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1669510097,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hua is a former ML Engineer and currently working on his PhD at Northeastern University. His industrial experience includes large-scale data engineering, big data, product search and recommender system. His research concentrates on (deep) reinforcement learning, stochastic optimization and (dynamic) Bayesian network.","tags":null,"title":"Hua Zheng","type":"authors"},{"authors":["Hua Zheng","Sarah W Harcum","Jinxiang Pei","Wei Xie"],"categories":["Stochastic Modeling","Computational Biology"],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705033628,"objectID":"c6c3fe55898403bffaf988c061ae5ce6","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2024-stochastic/","publishdate":"2024-01-12T04:26:55.335149Z","relpermalink":"/hua.zheng/publication/zheng-2024-stochastic/","section":"publication","summary":"Large-scale manufacturing of induced pluripotent stem cells (iPSCs) is essential for cell therapies and regenerative medicines. Yet, iPSCs form large cell aggregates in suspension bioreactors, resulting in insufficient nutrient supply and extra metabolic waste build-up for the cells located at the core. Since subtle changes in micro-environment can lead to a heterogeneous cell population, a novel Biological System-of-Systems (Bio-SoS) framework is proposed to model cell-to-cell interactions, spatial and metabolic heterogeneity, and cell response to micro-environmental variation. Building on stochastic metabolic reaction network, aggregation kinetics, and reaction-diffusion mechanisms, the Bio-SoS model characterizes causal interdependencies at individual cell, aggregate, and cell population levels. It has a modular design that enables data integration and improves predictions for different monolayer and aggregate culture processes. In addition, a variance decomposition analysis is derived to quantify the impact of factors (i.e., aggregate size) on cell product health and quality heterogeneity.","tags":["induced pluripotent stem cells","aggregate culture dynamics","multi-scale modelling","stochastic metabolic network","reaction-diffusion model","population balance modelling","metabolic heterogeneity"],"title":"Stochastic biological system-of-systems modelling for iPSC culture","type":"publication"},{"authors":["Hua Zheng","Kuang-Hung Liu","Igor Fedorov","Xin Zhang","Wen-Yen Chen","Wei Wen"],"categories":[],"content":"","date":1700611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652121654,"objectID":"aabff618a6b4347e39cc863e2314cabe","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2024-sigeo/","publishdate":"2023-11-22T00:00:00Z","relpermalink":"/hua.zheng/publication/zheng-2024-sigeo/","section":"publication","summary":"Neural Architecture Search (NAS) has become a widely used tool for automating neural network design. While one-shot NAS methods have successfully reduced computational requirements, they often require extensive training. On the other hand, zero-shot NAS utilizes training-free proxies to evaluate a candidate architecture test performance but has two limitations (1) inability to use the information gained as a network improves with training and (2) unreliable performance, particularly in complex domains like RecSys, due to the multi-modal data inputs and complex architecture configurations. To synthesize the benefits of both methods, we introduce a sub-one-shot paradigm that serves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the supernet is trained using only a small subset of the training data, a phase we refer to as warm-up. Within this framework, we present SiGeo, a proxy founded on a novel theoretical framework that connects the supernet warm-up with the efficacy of the proxy. Extensive experiments have shown that SiGeo, with the benefit of warm-up, consistently outperforms state-of-the-art NAS proxies on various established NAS benchmarks. When a supernet is warmed up, it can achieve comparable performance to weight-sharing one-shot NAS methods, but with a significant reduction (∼60\\%) in computational costs.","tags":["Machine Learning (cs.LG)"],"title":"SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape","type":"publication"},{"authors":["Hua Zheng","Wei Xie","Ilya O Ryzhov","Dongming Xie"],"categories":["Policy Optimization","Bayesian Network"],"content":"","date":1665446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669510097,"objectID":"ceb7b52aadfba46c23881dc2fed69210","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-policy/","publishdate":"2022-10-11T00:48:17.46984Z","relpermalink":"/hua.zheng/publication/zheng-2021-policy/","section":"publication","summary":"Biopharmaceutical manufacturing is a rapidly growing industry with impact in virtually all branches of medicine. Biomanufacturing processes require close monitoring and control, in the presence of complex bioprocess dynamics with many interdependent factors, as well as extremely limited data due to the high cost of experiments and the novelty of personalized bio-drugs. We develop a new model-based reinforcement learning framework that can achieve human-level control in low-data environments. A dynamic Bayesian network is used to capture causal interdependencies between factors and predict how the effects of different inputs propagate through the pathways of the bioprocess mechanisms. This model is interpretable and enables the design of process control policies that are robust against model risk. We present a computationally efficient, provably convergent stochastic gradient method for optimizing such policies. Validation is conducted on a realistic application with a multidimensional, continuous state variable.","tags":["Reinforcement Learning","Policy Optimization","Hybrid Model","Biopharmaceutical Manufacturing","Process Control"],"title":"Policy Optimization in Bayesian Network Hybrid Models of Biomanufacturing Processes","type":"publication"},{"authors":["Hua Zheng","Wei Xie","M Ben Feng"],"categories":["Policy Optimization","Variance Reduction","Experience Replay"],"content":"Introduction Experience replay allows agents to remember and reuse historical transitions. However, the uniform reuse strategy regardless of their significance is implicitly biased toward out-of-date observations. To overcome this limitation, we propose a general variance reduction based experience reply (VRER) approach, which allows policy optimization algorithms to selectively reuse the most relevant samples and improve policy gradient estimation. It tends to put more weight on historical observations that are more likely sampled from the target distribution. Different from other ER methods VRER is a theoretically justified and simple-to-use approach. Our theoretical and empirical studies demonstrate that the proposed VRER can accelerate the learning of optimal policy and enhance the performance of state-of-the-art policy optimization approaches.Experience replay allows agents to remember and reuse historical transitions. However, the uniform reuse strategy regardless of their significance is implicitly biased toward out-of-date observations. To overcome this limitation, we propose a general variance reduction based experience reply (VRER) approach, which allows policy optimization algorithms to selectively reuse the most relevant samples and improve policy gradient estimation. It tends to put more weight on historical observations that are more likely sampled from the target distribution. Different from other ER methods VRER is a theoretically justified and simple-to-use approach. Our theoretical and empirical studies demonstrate that the proposed VRER can accelerate the learning of optimal policy and enhance the performance of state-of-the-art policy optimization approaches.\nOpen-source Library: vrer-pg vrer-pg is a tensorflow based AI library which facilitates experimentation with existing policy optimization algorithms with variance reduction based experience replay. It provides well tested components that can be easily modified or extended. The available selection of algorithms can be used directly or through command line.\nvrer-pg Results The performance improvement of state-of-the-art PO algorithms after using VRER. Results are described by the mean performance curves and 95% confidence intervals of PPO(-VRER), TRPO(-VRER) and VPG(-VRER).\n","date":1662681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662741015,"objectID":"a67e5aecec95c67fa55422da9a723d4c","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-green/","publishdate":"2021-10-26T16:30:15.537584Z","relpermalink":"/hua.zheng/publication/zheng-2021-green/","section":"publication","summary":"Can a reinforcement learning (RL) agent remember and learn from the past, just like a human? Short answer: \"yes but only after selecting relevant and valuable experiences from the memory.\" I am glad to announce my new paper and its open-source project, both built on my research about \"variance reduction based experience replay\" (VRER). Long story short, VRER is a generic experience replay method with provable sample efficiency. **VRER makes the reinforcementlearning agent remember by selectively replaying past experiences**. This selective mechanism can adaptively filter out samples that are outdated, irrelevant and unstable. Our empirical study shows that VRER substantially improves the state-of-the-art policy optimization algorithms, such as trust region policy optimization and proximal policy optimization, in both convergence speed and robustness.","tags":["Reinforcement Learning","Importance Sampling","Experience Replay","Policy Optimization","Variance Reduction"],"title":"Variance Reduction based Experience Replay for Policy Optimization","type":"publication"},{"authors":["Hua Zheng"],"categories":["Blog"],"content":"I am recently encountering some theoretical problems during a reinforcement learning (RL) research project. I started to realize how much difference between how people use versus how people prove RL algorithms. Although rarely mentioned in the literature, some gaps indeed exist and may be useful to think a little deeper. It motivates me to write some of my thoughts down.\nNote: Some of the practical heuristics might already have theoretical justifications that I do not know or bypassed by some reasonable assumptions. Thus, please correct me if I misunderstand or have mistakes.\nExperience Replay Cause Bias Off-policy learning is a large category of RL algorithms. it, instead of following the target policy, enables a target policy to be learned by using data from behavior policies. Experience replay is central to off-policy algorithms in deep reinforcement learning (RL). As the fundamental data-generating mechanism in off-policy deep reinforcement learning, it has been shown to improve sample efficiency and stability by storing and randomly replaying a fixed number of the most recently collected transitions for training.\nOne interesting problem of experience replay is that the stored transitions are inevitably dependent with the current/target policy or value function, because the policy/value function parameter is in fact constructed from those transitions. In other words, all transitions are correlated. It violates the following conditional unbiasedness assumption in stochastic policy gradient methods.\nThe estimated gradient is unbiased condition on the history (filtration) $\\mathcal{F}_k=\\{\\xi_1,\\ldots,\\xi_{k};\\theta_0,\\theta_1,\\ldots,\\theta_{k}\\}$, such that $\\mathbb{E}_\\xi[g(\\theta_k,\\xi)|\\mathcal{F}_{k}]=\\nabla f(\\theta_k)$. ($\\mathcal{F}_{k}$ denotes the outcomes up to and including time $k$). In the classical stochastic gradient descent (SGD), the data are assumed to be sampled from a unknown population, which is also referred as to the behavior distribution in off-policy batch RL. However, when you have a growing experience replay buffer in which transitions were continuously collected by following past policies, you won’t have a fixed unknown behavior distribution but a changing known one. The main concerns behind this conditional dependence makes theoretical study of experience replay difficult.\nCompatibility of Critic Model Compatibility is an important concept in actor-critic methods, which are a popular class of on-policy RL algorithms for computing an optimal policy. The actor-critic consists of two eponymous components. An actor adjusts the parameters of the stochastic policy by stochastic gradient ascent. A critic estimates the action-value function to evaluate the action chosen by current policy. Instead of the unknown true action-value function, an action-value function is used, which introduce the concept of compatibility.\nIn general, substituting a function approximator for the true action-value (or value) function may introduce bias. However, the bias is zero if the function approximator is compatible, i.e.\nthe function approximator is linear in the score function of policy parameters of the function approximator are chosen to minimize the mean-squared error of approximated and true action-value function. In practice, condition 2 is usually relaxed in favor of policy evaluation algorithms that estimate the value function more efficiently by temporal-difference learning. However, condition 1 is rarely satisfied. This issues applies to almost all, if not all, modern deep policy optimization algorithms.\nGood news is that people has released this issue and some literature are working on relaxing the compatibility constraint for neural network [1-3].\nDiscounted Objective I had recently revisited the policy gradient theorem proof when working on my actor-critic related paper, which leads to this question.\nAfter a quick literature review I found Russo Alessio mentioned the same issue in his blog [4] and Nota and Thomas [5] had raised concerns about the similar issue by constructing a counterexample. In addition, Naik et al [6] criticize discounted objective as well but in a different context. I personally used a lot of discounted objective, but still felt unclear on some concepts. Part of the reason comes from inaccurate or ambiguous description and the other part I think comes from the fact that RL is an interdisciplinary area including contribution from optimization, operations research, statistics and computer science. As a result, people used different terminologies and notations, which further exaggerates the ambiguity.\nNow let’s briefly discuss the issue in infinite-horizon MDP setting. For detailed discussion, I recommend reading the blog and literature mentioned above.\nIn the classic policy gradient theorem, the discounted policy gradient, tells us how to modify the policy parameters, $\\theta$, in order to increase $\\nabla J_\\gamma(\\theta)$ , and is given by:\n$$ \\begin{equation} …","date":1651968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651968000,"objectID":"cead3918ea288d986cfa505c2fcd759b","permalink":"https://zhenghuazx.github.io/hua.zheng/post/somegapsrl/","publishdate":"2022-05-08T00:00:00Z","relpermalink":"/hua.zheng/post/somegapsrl/","section":"post","summary":"Some concerns on reinforcement learning algorithms.","tags":["Reinforcement Learning"],"title":"Some Gaps Between Reinforcement Learning Practice and Theory","type":"post"},{"authors":["Hua Zheng"],"categories":["Tutorial","Process Modeling","Dynamic Bayesian Network","Hybrid Model","Inference"],"content":"Table of Contents Introduction Motivation Process Modeling Linear Gaussian Dynamic Bayesian Network Based Hybrid Model Process Trajectory/Episode Distribution Model Formulation Inference Nonlinear Dynamic Bayesian Network Based Hybrid Model Parametric Variation Modeling Latent State Modeling Inference (A Bayesian Approach) Conclusion Next? Reference Introduction This is a tutorial about a series of advanced hybrid process models for people with some background in mathematical and statistical modeling and interested in the process modeling. The class of process models I will present in this tutorial is mainly based on Bayesian network and Bayesian inference.\nMotivation For many biochemical/physical/economic processes, the laws of physics and biochemistry often allow us to construct mechanistic models in forms of ordinary/partial/algebraic differential equations. While these models can well describe the the mean changes in a reaction/movement/economy profile over time, these models often ignore the impact from various sources of process inherent stochasticity. For example, in a cell culture process, although batch-to-batch variation and bioprocess noise are often dominant sources of process variation [1], biochemical kinetics literature rarely incorporates them into the ordinary/partial differential equation (ODE/PDE) based mechanistic models. In a study on the microbial cell-to-cell phenotypic diversity, [2] identified the intracellular production fluctuations as one of the major sources of the bioprocessing noise. Raw material variability is another critical source of uncertainty impacting cell cultures [3]. In fact, process noises are widely seen in most complex systems such as economy, cell metabolism, production process of mRNA vaccines. Therefore, it is of great importance to understand and incorporate major sources of stochastic uncertainty into a process model.\nProcess Modeling Process-based modeling is a modeling framework for complex systems. In contrast to agent-based models and other mainstream languages in complex systems, in a process-based model the structural features of a system are encoded in the interactions between the entities, rather than in the entities themselves.\nOrdinary Differential Equations Based Mechanistic Model A control system is a dynamical system on which one can act by using suitable controls. In this article, the dynamical model is modeled by ordinary differential equations of the following type\n$$ \\dot{\\pmb{s}}=\\pmb f(\\pmb s,\\pmb a)\\tag{1}\\label{eq: 1} $$ where $\\pmb{f}(\\cdot)$ encodes the causal interdependencies between actions and state. One typically assumes that the functional form of $\\pmb{f}$ is known, though it may also depend on additional parameters calibrated from data. The variable $\\pmb s$ is the state and belongs to some space $\\mathcal{S}$. The variable $\\pmb{a}$ is the control and belongs to some space $\\mathcal{A}$. In this article, the space $\\mathcal{S}$ is of infinite dimension.\nRemark:\nIn control literature, people use the state and control variables $(x,u)$ instead of state and action $(\\pmb{s},\\pmb{a})$. This is a first order differential equation; $\\pmb f(\\cdot)=\\left(f_1,\\ldots, f_n\\right)(\\cdot)=\\left(f_1(\\cdot),\\ldots, f_n(\\cdot)\\right)$ is a vector of functions. Some characteristics of the dynamical system can be used to analyze the Eq.(1), such as Stability: means that the trajectories do not change too much under small perturbations. Asymptotically stability controllability: Example: the kinetic/dynamic equations is linear, e.g. $\\dot{s}=\\mu s(t)$, where $\\mu$ is a constant, and in this case, $f(x)=ax$. The stability of linear systems is straightforward, i.e. stable if $\\mu\u0026lt;0$, and unstable otherwise.\nLinear Gaussian Dynamic Bayesian Network Based Hybrid Model [4] We model the process dynamics as a finite-horizon Markov decision process (MDP) specified by $(\\mathcal{S}, \\mathcal{A}, H, r, p)$, where $\\mathcal{S}$, $\\mathcal{A}$, $H$, $r$ and $p$ represent state space, action space, planning horizon, reward function, and state transition probability model. The process state transition is modeled as, $$ \\begin{equation} \\pmb{s}_{t+1} \\sim p(\\pmb{s}_{t+1}|\\pmb{s}_t,\\pmb{a}_t;\\pmb\\theta_t) \\end{equation} $$ where $\\pmb{s}_t\\in \\mathcal{S}\\subset \\mathbb{R}^n$ denotes the process state (e.g., glucose and lactate concentrations and cell density), $\\pmb{a}_t \\in \\mathcal{A}\\subset\\mathbb{R}^m$ is the action (also known as control inputs) at time step $t\\in\\mathcal{H}$. Here $\\mathcal{H}\\equiv{1,2,\\ldots,H}$ denotes the discrete time index (a.k.a. decision epochs).\nProcess Trajectory/Episode Distribution Then, the distribution of the entire trajectory $\\pmb\\tau=(\\pmb{s}_1,\\pmb{a}_1,\\pmb{s}_2,\\pmb{a}_2,\\ldots,\\pmb{s}_{H})$ of the process can be written as a product\n$$ p(\\pmb{\\tau}) = p(\\pmb{s}_1)\\prod_{t=1}^{H-1} p(\\pmb{s}_{t+1}|\\pmb{s}_t,\\pmb{a}_t) p(\\pmb{a}_t)\\tag{2}\\label{eq:trajectory-distribution} $$ of conditional distributions. Given a set of …","date":1651708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651708800,"objectID":"ffa4ec6723d9c0d6aba23ce83b9054fc","permalink":"https://zhenghuazx.github.io/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/","publishdate":"2022-05-05T00:00:00Z","relpermalink":"/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/","section":"post","summary":"This is a tutorial about a series of advanced hybrid process models for people with some background in mathematical and statistical modeling and interested in the process modeling.","tags":["Process Modeling","Dynamic Bayesian Network","Hybrid Model"],"title":"Dynamic Bayesian Network Based Hybrid Process Model","type":"post"},{"authors":["Hua Zheng","Wei Xie","Keqi Wang","Zheng Li"],"categories":null,"content":"","date":1650895200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650895200,"objectID":"de66b005fafa9b69174ce7562f2a8639","permalink":"https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-2/","publishdate":"2022-04-25T22:48:49-04:00","relpermalink":"/hua.zheng/talk/32nd-annual-poms-conference-talk-2/","section":"event","summary":"Knowledge Graph Hybrid Model-based Bayesian Reinforcement Learning for Cell Therapy Manufacturing Process Control","tags":["Hybrid Model","Reinforcement Learning","Cell Therapy","Process Control"],"title":"32nd Annual POMS-Conference (Talk 2)","type":"event"},{"authors":null,"categories":null,"content":"vLab is a package for biopharmaceutical manufacturing simulation in Python. It utilizes the diffeqpy, a python wrapper of DifferentialEquations.jl and scipy built-in ode solver for its core routines to give high performance solving of large stiff differential equations, currently including:\nN-linked Gycosylation Simulation in Perfusion Bioreactor Raman Spectrometry and Raman Data Analytics Plantwise Simulation (Cell Culture and Chromatography) The end goal of this project is to build a modularized platform, composed of an interactive interface and a digital twin based “vLab”. The digital twin is designed to simulate end-to-end bioprocess by using state-of-the-art mechanistic models for prediction, control and risk management.\n","date":1650844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650844800,"objectID":"57cf098899f0b447d1e1f48b0eac600c","permalink":"https://zhenghuazx.github.io/hua.zheng/project/vlab/","publishdate":"2022-04-25T00:00:00Z","relpermalink":"/hua.zheng/project/vlab/","section":"project","summary":"A Digital Twin and Simulation Library for Biopharmaceutical Manufacturing.","tags":["Digital Twin","Ongoing"],"title":"Virtual Laboratory","type":"project"},{"authors":["Hua Zheng","Wei Xie"],"categories":null,"content":"","date":1650731400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650731400,"objectID":"688d7a290dcd1f78fcf1320f3a08b8c9","permalink":"https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-1/","publishdate":"2022-04-25T23:05:30-04:00","relpermalink":"/hua.zheng/talk/32nd-annual-poms-conference-talk-1/","section":"event","summary":"Variance Reduced Experience Replay for Policy Optimization with Partial Trajectory Reuse","tags":["Variance Reduced Experience Replay","Policy Optimization"],"title":"32nd Annual POMS-Conference (Talk 1)","type":"event"},{"authors":["Wei Xie","Keqi Wang","Hua Zheng","Ben Feng"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651858206,"objectID":"6d22207b46ebd3a3074ed68cefa4ff60","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/xie-2022-dynamic/","publishdate":"2022-05-06T17:30:06.499328Z","relpermalink":"/hua.zheng/publication/xie-2022-dynamic/","section":"publication","summary":"Driven by the critical needs of biomanufacturing 4.0, we present a probabilistic knowledge graph hybrid model characterizing complex spatial-temporal causal interdependencies of underlying bioprocessing mechanisms. It can faithfully capture the important properties, including nonlinear reactions, partially observed state, and nonstationary dynamics. Given limited process observations, we derive a posterior distribution quantifying model uncertainty, which can facilitate mechanism learning and support robust process control. To avoid evaluation of intractable likelihood, Approximate Bayesian Computation sampling with Sequential Monte Carlo (ABC-SMC) is developed to approximate the posterior distribution. Given high stochastic and model uncertainties, it is computationally expensive to match process output trajectories. Therefore, we propose a linear Gaussian dynamic Bayesian network (LG-DBN) auxiliary likelihood-based ABC-SMC algorithm. Through matching observed and simulated summary statistics, the proposed approach can dramatically reduce the computation cost and accelerate the posterior approximation convergence.","tags":["Bayesian Inference","Hybrid Model","Approximate Bayesian Computation","Auxiliary Likelihood-based Summary Statistics"],"title":"Dynamic Bayesian Network Auxiliary ABC-SMC for Hybrid Model Bayesian Inference to Accelerate Biomanufacturing Process Mechanism Learning and Robust Control","type":"publication"},{"authors":["Hua Zheng","Wei Xie","Keqi Wang","Zheng Li"],"categories":["Reinforcement Learning","Hybrid Model"],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650581831,"objectID":"1311faf374278b6f35a6697271d4b902","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-opportunities/","publishdate":"2022-04-21T22:57:11.799443Z","relpermalink":"/hua.zheng/publication/zheng-2022-opportunities/","section":"publication","summary":"","tags":["Reinforcement Learning","Hybrid Model","Bayesian Statistics","Cell Therapy","Process Control"],"title":"Opportunities of Hybrid Model-based Reinforcement Learning for Cell Therapy Manufacturing Process Development and Control","type":"publication"},{"authors":["Hua Zheng","Wei Xie"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652121654,"objectID":"897bdc6edc3a78c2f48aece29c59008e","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-variance/","publishdate":"2022-05-09T18:40:54.543907Z","relpermalink":"/hua.zheng/publication/zheng-2022-variance/","section":"publication","summary":"","tags":["\"Machine Learning (cs.LG)\"","\"FOS: Computer and information sciences\"","\"FOS: Computer and information sciences\""],"title":"Variance Reduction based Partial Trajectory Reuse to Accelerate Policy Gradient Optimization","type":"publication"},{"authors":["Wei Xie","Yuan Yi","Hua Zheng"],"categories":["Stochastic Programming"],"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623632184,"objectID":"c71a47e2b4d1a4a3a2deb36a07cc98f4","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/10-1145-3411080/","publishdate":"2021-06-14T00:56:23.815267Z","relpermalink":"/hua.zheng/publication/10-1145-3411080/","section":"publication","summary":"To integrate strategic, tactical, and operational decisions, stochastic programming has been widely used to guide dynamic decision-making. In this article, we consider complex systems and introduce the global-local metamodel-assisted stochastic programming via simulation that can efficiently employ the simulation resource to iteratively solve for the optimal first- and second-stage decisions. Specifically, at each visited first-stage decision, we develop a local metamodel to simultaneously solve a set of scenario-based second-stage optimization problems, which also allows us to estimate the optimality gap. Then, we construct a global metamodel accounting for the errors induced by: (1) using a finite number of scenarios to approximate the expected future cost occurring in the planning horizon, (2) second-stage optimality gap, and (3) finite visited first-stage decisions. Assisted by the global-local metamodel, we propose a new simulation optimization approach that can efficiently and iteratively search for the optimal first- and second-stage decisions. Our framework can guarantee the convergence of optimal solution for the discrete two-stage optimization with unknown objective, and the empirical study indicates that it achieves substantial efficiency and accuracy.","tags":["\"dynamic decision-making\"","\"two-stage optimization\"","\"Simulation optimization\"","\"stochastic programming\"","\"Gaussian process metamodel\""],"title":"Global-Local Metamodel-Assisted Stochastic Programming via Simulation","type":"publication"},{"authors":["Hua Zheng","Wei Xie","M. Ben Feng"],"categories":null,"content":"","date":1635238800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635238800,"objectID":"7be1a7a5df64312337b056e6fc9de2b0","permalink":"https://zhenghuazx.github.io/hua.zheng/talk/informs-annual-2021/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/hua.zheng/talk/informs-annual-2021/","section":"event","summary":"Green Simulation Assisted Policy Gradient to Accelerate Stochastic Process Control.","tags":[],"title":"INFORMS Annual 2021","type":"event"},{"authors":["Wei Xie","Beverly Kris Jaeger-Helton","Jared Auclair","Jinxiang Pei","Hua Zheng"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623632571,"objectID":"ef209685ce3c8bb1e2f6bd925ac0e0ae","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/xie-2020-stem/","publishdate":"2021-06-14T01:02:50.207041Z","relpermalink":"/hua.zheng/publication/xie-2020-stem/","section":"publication","summary":"","tags":["Education"],"title":"STEM Education and Industry Workforce Life-Long Training Platform Development to Faciliate Smart Biopharmaceutical Manufacturing 4.0-4","type":"publication"},{"authors":["Hua Zheng","Ilya O. Ryzhov","Wei Xie","Judy Zhong"],"categories":["Deep Q-Netowrk","Electronic Health Records","Multimorbidity Management"],"content":"Prescription by AI Doctor We believe the potential for the use of AI is dramatic especially as it can assist family clinicians who are usually overloaded with patients to make better choices for treatment of their type 2 patients in order to help prevent hyperglycemia, hypertension and CV risk outcomes. ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623631605,"objectID":"462def7903cf843d8f6982b2042a57fe","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021/","publishdate":"2021-06-14T00:46:45.752236Z","relpermalink":"/hua.zheng/publication/zheng-2021/","section":"publication","summary":"**Background**: Comorbid chronic conditions are common among people with type 2 diabetes. We developed an artificial intelligence algorithm, based on reinforcement learning (RL), for personalized diabetes and multimorbidity management, with strong potential to improve health outcomes relative to current clinical practice. **Methods**: We modeled glycemia, blood pressure, and cardiovascular disease (CVD) risk as health outcomes, using a retrospective cohort of 16,665 patients with type 2 diabetes from New York University Langone Health ambulatory care electronic health records in 2009-2017. We trained an RL prescription algorithm that recommends a treatment regimen optimizing patients' cumulative health outcomes using their individual characteristics and medical history at each encounter. The RL recommendations were evaluated on an independent subset of patients. **Results**: The single-outcome optimization RL algorithms, RL-glycemia, RL-blood pressure, and RL-CVD, recommended consistent prescriptions as that observed by clinicians in 86.1%, 82.9%, and 98.4% of the encounters, respectively. For patient encounters in which the RL recommendations differed from the clinician prescriptions, significantly fewer encounters showed uncontrolled glycemia (A1c \u003e8% in 35% of encounters), uncontrolled hypertension (blood pressure \u003e 140mmHg in 16% of encounters), and high CVD risk (risk \u003e20% in 25% of encounters) under RL algorithms compared with those observed under clinicians (43%, 27%, and 31% of encounters, respectively; all p-values \u003c= 0.001). **Conclusion**: A personalized RL prescriptive framework for type 2 diabetes yielded high concordance with clinicians' prescriptions, and substantial improvements in glycemia, blood pressure, and CVD risk outcomes.","tags":["Reinforcement Learning","Medical Research"],"title":"Personalized Multimorbidity Management for Patients with Type 2 Diabetes Using Reinforcement Learning of Electronic Health Records","type":"publication"},{"authors":null,"categories":null,"content":"","date":1613001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613001600,"objectID":"a836bc97c94ce94c8b202e5914d8cf39","permalink":"https://zhenghuazx.github.io/hua.zheng/project/ehr-rl/","publishdate":"2021-02-11T00:00:00Z","relpermalink":"/hua.zheng/project/ehr-rl/","section":"project","summary":"Personalized multimorbidity management for patients with type 2 diabetes using reinforcement learning of electronic health records.","tags":["Reinforcement Learning","Medical Research"],"title":"EHR-RL","type":"project"},{"authors":["Hua Zheng","Jiahao Zhu","Wei Xie","Judy Zhong"],"categories":["Reinforcement Learning"],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650586931,"objectID":"d7da928484a0765ddad3c4015e74f2b9","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-reinforcement/","publishdate":"2022-04-22T00:22:11.634209Z","relpermalink":"/hua.zheng/publication/zheng-2021-reinforcement/","section":"publication","summary":"**Background:** Patients with severe Coronavirus disease 19 (COVID-19) typically require supplemental oxygen as an essential treatment. We developed a machine learning algorithm, based on deep Reinforcement Learning (RL), for continuous management of oxygen flow rate for critically ill patients under intensive care, which can identify the optimal personalized oxygen flow rate with strong potentials to reduce mortality rate relative to the current clinical practice. **Methods:** We modeled the oxygen flow trajectory of COVID-19 patients and their health outcomes as a Markov decision process. Based on individual patient characteristics and health status, an optimal oxygen control policy is learned by using deep deterministic policy gradient (DDPG) and real-time recommends the oxygen flow rate to reduce the mortality rate. We assessed the performance of proposed methods through cross validation by using a retrospective cohort of 1372 critically ill patients with COVID-19 from New York University Langone Health ambulatory care with electronic health records from April 2020 to January 2021. **Results:** The mean mortality rate under the RL algorithm is lower than the standard of care by 2.57% (95% CI: 2.08–3.06) reduction (P \u003c 0.001) from 7.94% under the standard of care to 5.37% under our proposed algorithm. The averaged recommended oxygen flow rate is 1.28 L/min (95% CI: 1.14–1.42) lower than the rate delivered to patients. Thus, the RL algorithm could potentially lead to better intensive care treatment that can reduce the mortality rate, while saving the oxygen scarce resources. It can reduce the oxygen shortage issue and improve public health during the COVID-19 pandemic. **Conclusions:** A personalized reinforcement learning oxygen flow control algorithm for COVID-19 patients under intensive care showed a substantial reduction in 7-day mortality rate as compared to the standard of care. In the overall cross validation cohort independent of the training data, mortality was lowest in patients for whom intensivists’ actual flow rate matched the RL decisions.","tags":["Reinforcement Learning","Medical Research","Process Control"],"title":"Reinforcement Learning Assisted Oxygen Therapy for COVID-19 Patients Under Intensive Care","type":"publication"},{"authors":["Hua Zheng","Wei Xie","M. Ben Feng"],"categories":null,"content":"","date":1608028200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608028200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://zhenghuazx.github.io/hua.zheng/talk/winter-simulation-conference-2020/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/hua.zheng/talk/winter-simulation-conference-2020/","section":"event","summary":"Green Simulation Assisted Reinforcement Learning with Model Risk for Biomanufacturing Learning and Control.","tags":["Hybrid Model","Reinforcement Learning"],"title":"Winter Simulation Conference 2020","type":"event"},{"authors":["Hua Zheng","Wei Xie","M. Ben Feng"],"categories":["Policy Optimization","Biopharmaceutical Manufacturing"],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623631792,"objectID":"72354c6044d8c7227d4e718e634bfe43","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/10-5555-3466184-3466221/","publishdate":"2021-06-14T00:49:51.969067Z","relpermalink":"/hua.zheng/publication/10-5555-3466184-3466221/","section":"publication","summary":"Biopharmaceutical manufacturing faces critical challenges, including complexity, high variability, lengthy lead time, and limited historical data and knowledge of the underlying system stochastic process. To address these challenges, we propose a green simulation assisted model-based reinforcement learning to support process online learning and guide dynamic decision making. Basically, the process model risk is quantified by the posterior distribution. At any given policy, we predict the expected system response with prediction risk accounting for both inherent stochastic uncertainty and model risk. Then, we propose green simulation assisted reinforcement learning and derive the mixture proposal distribution of decision process and likelihood ratio based metamodel for the policy gradient, which can selectively reuse process trajectory outputs collected from previous experiments to increase the simulation data-efficiency, improve the policy gradient estimation accuracy, and speed up the search for the optimal policy. Our numerical study indicates that the proposed approach demonstrates the promising performance.","tags":["Green Simulation","Reinforcement Learning","Biopharmaceutical Manufacturing"],"title":"Green Simulation Assisted Reinforcement Learning with Model Risk for Biomanufacturing Learning and Control","type":"publication"},{"authors":null,"categories":null,"content":"","date":1521936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521936000,"objectID":"a72d3248e856c05acd99e1fba2ddbaf4","permalink":"https://zhenghuazx.github.io/hua.zheng/project/toxiccomment/","publishdate":"2018-03-25T00:00:00Z","relpermalink":"/hua.zheng/project/toxiccomment/","section":"project","summary":"A multi-headed DL model, capable of detecting different types of toxic comments like threats, obscenity, insults, and identity-based hate. It ranked 116 out of 4,539 teams with the private LB score 0.98689.","tags":["Deep Learning","Kaggle"],"title":"Toxic Comment Classification","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://zhenghuazx.github.io/hua.zheng/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/hua.zheng/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]