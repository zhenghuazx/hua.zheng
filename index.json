[{"authors":null,"categories":null,"content":"Hua is a former ML Engineer and currently working on his PhD at Northeastern University. His industrial experience includes large-scale data engineering, big data, product search and recommender system. His research concentrates on (deep) reinforcement learning, stochastic optimization and (dynamic) Bayesian network. With years of research and industry experience, he has built a multi-disciplinary skill set across computer science, optimization, machine learning, and statistics.\n  Download my resumé.\n","date":1651968000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1651968000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hua is a former ML Engineer and currently working on his PhD at Northeastern University. His industrial experience includes large-scale data engineering, big data, product search and recommender system. His research concentrates on (deep) reinforcement learning, stochastic optimization and (dynamic) Bayesian network.","tags":null,"title":"Hua Zheng","type":"authors"},{"authors":["Hua Zheng"],"categories":["Blog"],"content":"I am recently encountering some theoretical problems during a reinforcement learning (RL) research project. I started to realize how much difference between how people use versus how people prove RL algorithms. Although rarely mentioned in the literature, some gaps indeed exist and may be useful to think a little deeper. It motivates me to write some of my thoughts down.\nNote: Some of the practical heuristics might already have theoretical justifications that I do not know or bypassed by some reasonable assumptions. Thus, please correct me if I misunderstand or have mistakes.\nExperience Replay Cause Bias Off-policy learning is a large category of RL algorithms. it, instead of following the target policy, enables a target policy to be learned by using data from behavior policies. Experience replay is central to off-policy algorithms in deep reinforcement learning (RL). As the fundamental data-generating mechanism in off-policy deep reinforcement learning, it has been shown to improve sample efficiency and stability by storing and randomly replaying a fixed number of the most recently collected transitions for training.\nOne interesting problem of experience replay is that the stored transitions are inevitably dependent with the current/target policy or value function, because the policy/value function parameter is in fact constructed from those transitions. In other words, all transitions are correlated. It violates the following conditional unbiasedness assumption in stochastic policy gradient methods.\n The estimated gradient is unbiased condition on the history (filtration) $\\mathcal{F}_k=\\{\\xi_1,\\ldots,\\xi_{k};\\theta_0,\\theta_1,\\ldots,\\theta_{k}\\}$, such that $\\mathbb{E}_\\xi[g(\\theta_k,\\xi)|\\mathcal{F}_{k}]=\\nabla f(\\theta_k)$. ($\\mathcal{F}_{k}$ denotes the outcomes up to and including time $k$).  In the classical stochastic gradient descent (SGD), the data are assumed to be sampled from a unknown population, which is also referred as to the behavior distribution in off-policy batch RL. However, when you have a growing experience replay buffer in which transitions were continuously collected by following past policies, you won’t have a fixed unknown behavior distribution but a changing known one. The main concerns behind this conditional dependence makes theoretical study of experience replay difficult.\nCompatibility of Critic Model Compatibility is an important concept in actor-critic methods, which are a popular class of on-policy RL algorithms for computing an optimal policy. The actor-critic consists of two eponymous components. An actor adjusts the parameters of the stochastic policy by stochastic gradient ascent. A critic estimates the action-value function to evaluate the action chosen by current policy. Instead of the unknown true action-value function, an action-value function is used, which introduce the concept of compatibility.\nIn general, substituting a function approximator for the true action-value (or value) function may introduce bias. However, the bias is zero if the function approximator is compatible, i.e.\n the function approximator is linear in the score function of policy parameters of the function approximator are chosen to minimize the mean-squared error of approximated and true action-value function.  In practice, condition 2 is usually relaxed in favor of policy evaluation algorithms that estimate the value function more efficiently by temporal-difference learning. However, condition 1 is rarely satisfied. This issues applies to almost all, if not all, modern deep policy optimization algorithms.\nGood news is that people has released this issue and some literature are working on relaxing the compatibility constraint for neural network [1-3].\nDiscounted Objective I had recently revisited the policy gradient theorem proof when working on my actor-critic related paper, which leads to this question.\nAfter a quick literature review I found Russo Alessio mentioned the same issue in his blog [4] and Nota and Thomas [5] had raised concerns about the similar issue by constructing a counterexample. In addition, Naik et al [6] criticize discounted objective as well but in a different context. I personally used a lot of discounted objective, but still felt unclear on some concepts. Part of the reason comes from inaccurate or ambiguous description and the other part I think comes from the fact that RL is an interdisciplinary area including contribution from optimization, operations research, statistics and computer science. As a result, people used different terminologies and notations, which further exaggerates the ambiguity.\nNow let’s briefly discuss the issue in infinite-horizon MDP setting. For detailed discussion, I recommend reading the blog and literature mentioned above.\nIn the classic policy gradient theorem, the discounted policy gradient, tells us how to modify the policy parameters, $\\theta$, in order to increase $\\nabla J_\\gamma(\\theta)$ , and is given by:\n$$ \\begin{equation} …","date":1651968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651968000,"objectID":"cead3918ea288d986cfa505c2fcd759b","permalink":"https://zhenghuazx.github.io/hua.zheng/post/somegapsrl/","publishdate":"2022-05-08T00:00:00Z","relpermalink":"/hua.zheng/post/somegapsrl/","section":"post","summary":"Some concerns on reinforcement learning algorithms.","tags":["Reinforcement Learning"],"title":"Some Gaps Between Reinforcement Learning Practice and Theory","type":"post"},{"authors":["Hua Zheng"],"categories":["Tutorial","Process Modeling","Dynamic Bayesian Network","Hybrid Model","Inference"],"content":"Table of Contents  Introduction Motivation Process Modeling  Linear Gaussian Dynamic Bayesian Network Based Hybrid Model  Process Trajectory/Episode Distribution Model Formulation Inference   Nonlinear Dynamic Bayesian Network Based Hybrid Model  Parametric Variation Modeling Latent State Modeling Inference (A Bayesian Approach)     Conclusion Next? Reference  Introduction This is a tutorial about a series of advanced hybrid process models for people with some background in mathematical and statistical modeling and interested in the process modeling. The class of process models I will present in this tutorial is mainly based on Bayesian network and Bayesian inference.\nMotivation For many biochemical/physical/economic processes, the laws of physics and biochemistry often allow us to construct mechanistic models in forms of ordinary/partial/algebraic differential equations. While these models can well describe the the mean changes in a reaction/movement/economy profile over time, these models often ignore the impact from various sources of process inherent stochasticity. For example, in a cell culture process, although batch-to-batch variation and bioprocess noise are often dominant sources of process variation [1], biochemical kinetics literature rarely incorporates them into the ordinary/partial differential equation (ODE/PDE) based mechanistic models. In a study on the microbial cell-to-cell phenotypic diversity, [2] identified the intracellular production fluctuations as one of the major sources of the bioprocessing noise. Raw material variability is another critical source of uncertainty impacting cell cultures [3]. In fact, process noises are widely seen in most complex systems such as economy, cell metabolism, production process of mRNA vaccines. Therefore, it is of great importance to understand and incorporate major sources of stochastic uncertainty into a process model.\nProcess Modeling Process-based modeling is a modeling framework for complex systems. In contrast to agent-based models and other mainstream languages in complex systems, in a process-based model the structural features of a system are encoded in the interactions between the entities, rather than in the entities themselves.\nOrdinary Differential Equations Based Mechanistic Model A control system is a dynamical system on which one can act by using suitable controls. In this article, the dynamical model is modeled by ordinary differential equations of the following type\n $$ \\dot{\\pmb{s}}=\\pmb f(\\pmb s,\\pmb a)\\tag{1}\\label{eq: 1} $$ where $\\pmb{f}(\\cdot)$ encodes the causal interdependencies between actions and state. One typically assumes that the functional form of $\\pmb{f}$ is known, though it may also depend on additional parameters calibrated from data. The variable $\\pmb s$ is the state and belongs to some space $\\mathcal{S}$. The variable $\\pmb{a}$ is the control and belongs to some space $\\mathcal{A}$. In this article, the space $\\mathcal{S}$ is of infinite dimension.\nRemark:\n In control literature, people use the state and control variables $(x,u)$ instead of state and action $(\\pmb{s},\\pmb{a})$. This is a first order differential equation; $\\pmb f(\\cdot)=\\left(f_1,\\ldots, f_n\\right)(\\cdot)=\\left(f_1(\\cdot),\\ldots, f_n(\\cdot)\\right)$ is a vector of functions. Some characteristics of the dynamical system can be used to analyze the Eq.(1), such as  Stability: means that the trajectories do not change too much under small perturbations. Asymptotically stability controllability:     Example: the kinetic/dynamic equations is linear, e.g. $\\dot{s}=\\mu s(t)$, where $\\mu$ is a constant, and in this case, $f(x)=ax$. The stability of linear systems is straightforward, i.e. stable if $\\mu\u0026lt;0$, and unstable otherwise.\n Linear Gaussian Dynamic Bayesian Network Based Hybrid Model [4] We model the process dynamics as a finite-horizon Markov decision process (MDP) specified by $(\\mathcal{S}, \\mathcal{A}, H, r, p)$, where $\\mathcal{S}$, $\\mathcal{A}$, $H$, $r$ and $p$ represent state space, action space, planning horizon, reward function, and state transition probability model. The process state transition is modeled as, $$ \\begin{equation} \\pmb{s}_{t+1} \\sim p(\\pmb{s}_{t+1}|\\pmb{s}_t,\\pmb{a}_t;\\pmb\\theta_t) \\end{equation} $$ where $\\pmb{s}_t\\in \\mathcal{S}\\subset \\mathbb{R}^n$ denotes the process state (e.g., glucose and lactate concentrations and cell density), $\\pmb{a}_t \\in \\mathcal{A}\\subset\\mathbb{R}^m$ is the action (also known as control inputs) at time step $t\\in\\mathcal{H}$. Here $\\mathcal{H}\\equiv{1,2,\\ldots,H}$ denotes the discrete time index (a.k.a. decision epochs).\nProcess Trajectory/Episode Distribution Then, the distribution of the entire trajectory $\\pmb\\tau=(\\pmb{s}_1,\\pmb{a}_1,\\pmb{s}_2,\\pmb{a}_2,\\ldots,\\pmb{s}_{H})$ of the process can be written as a product\n $$ p(\\pmb{\\tau}) = p(\\pmb{s}_1)\\prod_{t=1}^{H-1} p(\\pmb{s}_{t+1}|\\pmb{s}_t,\\pmb{a}_t) p(\\pmb{a}_t)\\tag{2}\\label{eq:trajectory-distribution} $$ of conditional …","date":1651708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651708800,"objectID":"ffa4ec6723d9c0d6aba23ce83b9054fc","permalink":"https://zhenghuazx.github.io/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/","publishdate":"2022-05-05T00:00:00Z","relpermalink":"/hua.zheng/post/dynamicbayesiannetworkbasedhybridprocessmodel/","section":"post","summary":"This is a tutorial about a series of advanced hybrid process models for people with some background in mathematical and statistical modeling and interested in the process modeling.","tags":["Process Modeling","Dynamic Bayesian Network","Hybrid Model"],"title":"Dynamic Bayesian Network Based Hybrid Process Model","type":"post"},{"authors":["Hua Zheng","Wei Xie","Keqi Wang","Zheng Li"],"categories":null,"content":"","date":1650895200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650895200,"objectID":"de66b005fafa9b69174ce7562f2a8639","permalink":"https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-2/","publishdate":"2022-04-25T22:48:49-04:00","relpermalink":"/hua.zheng/talk/32nd-annual-poms-conference-talk-2/","section":"event","summary":"Knowledge Graph Hybrid Model-based Bayesian Reinforcement Learning for Cell Therapy Manufacturing Process Control","tags":["Hybrid Model","Reinforcement Learning","Cell Therapy","Process Control"],"title":"32nd Annual POMS-Conference (Talk 2)","type":"event"},{"authors":null,"categories":null,"content":"vLab is a package for biopharmaceutical manufacturing simulation in Python. It utilizes the diffeqpy, a python wrapper of DifferentialEquations.jl and scipy built-in ode solver for its core routines to give high performance solving of large stiff differential equations, currently including:\n N-linked Gycosylation Simulation in Perfusion Bioreactor Raman Spectrometry and Raman Data Analytics Plantwise Simulation (Cell Culture and Chromatography)     The end goal of this project is to build a modularized platform, composed of an interactive interface and a digital twin based “vLab”. The digital twin is designed to simulate end-to-end bioprocess by using state-of-the-art mechanistic models for prediction, control and risk management.\n","date":1650844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650844800,"objectID":"57cf098899f0b447d1e1f48b0eac600c","permalink":"https://zhenghuazx.github.io/hua.zheng/project/vlab/","publishdate":"2022-04-25T00:00:00Z","relpermalink":"/hua.zheng/project/vlab/","section":"project","summary":"A Digital Twin and Simulation Library for Biopharmaceutical Manufacturing.","tags":["Digital Twin","Ongoing"],"title":"Virtual Laboratory","type":"project"},{"authors":["Hua Zheng","Wei Xie"],"categories":null,"content":"","date":1650731400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650731400,"objectID":"688d7a290dcd1f78fcf1320f3a08b8c9","permalink":"https://zhenghuazx.github.io/hua.zheng/talk/32nd-annual-poms-conference-talk-1/","publishdate":"2022-04-25T23:05:30-04:00","relpermalink":"/hua.zheng/talk/32nd-annual-poms-conference-talk-1/","section":"event","summary":"Variance Reduced Experience Replay for Policy Optimization with Partial Trajectory Reuse","tags":["Variance Reduced Experience Replay","Policy Optimization"],"title":"32nd Annual POMS-Conference (Talk 1)","type":"event"},{"authors":["Wei Xie","Keqi Wang","Hua Zheng","Ben Feng"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651858206,"objectID":"6d22207b46ebd3a3074ed68cefa4ff60","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/xie-2022-dynamic/","publishdate":"2022-05-06T17:30:06.499328Z","relpermalink":"/hua.zheng/publication/xie-2022-dynamic/","section":"publication","summary":"Driven by the critical needs of biomanufacturing 4.0, we present a probabilistic knowledge graph hybrid model characterizing complex spatial-temporal causal interdependencies of underlying bioprocessing mechanisms. It can faithfully capture the important properties, including nonlinear reactions, partially observed state, and nonstationary dynamics. Given limited process observations, we derive a posterior distribution quantifying model uncertainty, which can facilitate mechanism learning and support robust process control. To avoid evaluation of intractable likelihood, Approximate Bayesian Computation sampling with Sequential Monte Carlo (ABC-SMC) is developed to approximate the posterior distribution. Given high stochastic and model uncertainties, it is computationally expensive to match process output trajectories. Therefore, we propose a linear Gaussian dynamic Bayesian network (LG-DBN) auxiliary likelihood-based ABC-SMC algorithm. Through matching observed and simulated summary statistics, the proposed approach can dramatically reduce the computation cost and accelerate the posterior approximation convergence.","tags":["Bayesian Inference","Hybrid Model","Approximate Bayesian Computation","Auxiliary Likelihood-based Summary Statistics"],"title":"Dynamic Bayesian Network Auxiliary ABC-SMC for Hybrid Model Bayesian Inference to Accelerate Biomanufacturing Process Mechanism Learning and Robust Control","type":"publication"},{"authors":["Hua Zheng","Wei Xie","Keqi Wang","Zheng Li"],"categories":["Reinforcement Learning","Hybrid Model"],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650581831,"objectID":"1311faf374278b6f35a6697271d4b902","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-opportunities/","publishdate":"2022-04-21T22:57:11.799443Z","relpermalink":"/hua.zheng/publication/zheng-2022-opportunities/","section":"publication","summary":"","tags":["Reinforcement Learning","Hybrid Model","Bayesian Statistics","Cell Therapy","Process Control"],"title":"Opportunities of Hybrid Model-based Reinforcement Learning for Cell Therapy Manufacturing Process Development and Control","type":"publication"},{"authors":["Hua Zheng","Wei Xie"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652121654,"objectID":"897bdc6edc3a78c2f48aece29c59008e","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2022-variance/","publishdate":"2022-05-09T18:40:54.543907Z","relpermalink":"/hua.zheng/publication/zheng-2022-variance/","section":"publication","summary":"","tags":["\"Machine Learning (cs.LG)\"","\"FOS: Computer and information sciences\"","\"FOS: Computer and information sciences\""],"title":"Variance Reduction based Partial Trajectory Reuse to Accelerate Policy Gradient Optimization","type":"publication"},{"authors":["Wei Xie","Yuan Yi","Hua Zheng"],"categories":["Stochastic Programming"],"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623632184,"objectID":"c71a47e2b4d1a4a3a2deb36a07cc98f4","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/10-1145-3411080/","publishdate":"2021-06-14T00:56:23.815267Z","relpermalink":"/hua.zheng/publication/10-1145-3411080/","section":"publication","summary":"To integrate strategic, tactical, and operational decisions, stochastic programming has been widely used to guide dynamic decision-making. In this article, we consider complex systems and introduce the global-local metamodel-assisted stochastic programming via simulation that can efficiently employ the simulation resource to iteratively solve for the optimal first- and second-stage decisions. Specifically, at each visited first-stage decision, we develop a local metamodel to simultaneously solve a set of scenario-based second-stage optimization problems, which also allows us to estimate the optimality gap. Then, we construct a global metamodel accounting for the errors induced by: (1) using a finite number of scenarios to approximate the expected future cost occurring in the planning horizon, (2) second-stage optimality gap, and (3) finite visited first-stage decisions. Assisted by the global-local metamodel, we propose a new simulation optimization approach that can efficiently and iteratively search for the optimal first- and second-stage decisions. Our framework can guarantee the convergence of optimal solution for the discrete two-stage optimization with unknown objective, and the empirical study indicates that it achieves substantial efficiency and accuracy.","tags":["\"dynamic decision-making\"","\"two-stage optimization\"","\"Simulation optimization\"","\"stochastic programming\"","\"Gaussian process metamodel\""],"title":"Global-Local Metamodel-Assisted Stochastic Programming via Simulation","type":"publication"},{"authors":["Hua Zheng","Wei Xie","M. Ben Feng"],"categories":null,"content":"","date":1635238800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635238800,"objectID":"7be1a7a5df64312337b056e6fc9de2b0","permalink":"https://zhenghuazx.github.io/hua.zheng/talk/informs-annual-2021/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/hua.zheng/talk/informs-annual-2021/","section":"event","summary":"Green Simulation Assisted Policy Gradient to Accelerate Stochastic Process Control.","tags":[],"title":"INFORMS Annual 2021","type":"event"},{"authors":["Hua Zheng","Wei Xie","M Ben Feng"],"categories":["Policy Optimization"],"content":"","date":1635206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635265815,"objectID":"a67e5aecec95c67fa55422da9a723d4c","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-green/","publishdate":"2021-10-26T16:30:15.537584Z","relpermalink":"/hua.zheng/publication/zheng-2021-green/","section":"publication","summary":"This study is motivated by the critical challenges in the biopharmaceutical manufacturing, including high complexity, high uncertainty, and very limited process data. Each experiment run is often very expensive. To support the optimal and robust process control, we propose a general green simulation assisted policy gradient (GS-PG) framework for both online and offline learning settings. Basically, to address the key limitations of state-of-art reinforcement learning (RL), such as sample inefficiency and low reliability, we create a mixture likelihood ratio based policy gradient estimation that can leverage on the information from historical experiments conducted under different inputs, including process model coefficients and decision policy parameters. Then, to accelerate the learning of optimal and robust policy, we further propose a variance reduction based sample selection method that allows GS-PG to intelligently select and reuse most relevant historical trajectories. The selection rule automatically updates the samples to be reused during the learning of process mechanisms and the search for optimal policy. Our theoretical and empirical studies demonstrate that the proposed framework can perform better than the state-of-art policy gradient approach and accelerate the optimal robust process control for complex stochastic systems under high uncertainty. ","tags":["Reinforcement Learning","Importance Sampling"],"title":"Green Simulation Assisted Policy Gradient to Accelerate Stochastic Process Control","type":"publication"},{"authors":["Hua Zheng","Wei Xie","Ilya O. Ryzhov","Doming Xie"],"categories":["Policy Optimization","Bayesian Network"],"content":"","date":1620864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620864000,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/hua.zheng/publication/example/","section":"publication","summary":"Biopharmaceutical manufacturing is a rapidly growing industry with impact in virtually all branches of medicine. Biomanufacturing processes require close monitoring and control, in the presence of complex bioprocess dynamics with many interdependent factors, as well as extremely limited data due to the high cost and long duration of experiments. We develop a novel model-based reinforcement learning framework that can achieve human-level control in low-data environments. The model uses a probabilistic knowledge graph to capture causal interdependencies between factors in the underlying stochastic decision process, leveraging information from existing kinetic models from different unit operations while incorporating real-world experimental data. We then present a computationally efficient, provably convergent stochastic gradient method for policy optimization. Validation is conducted on a realistic application with a multi-dimensional, continuous state variable.","tags":["Reinforcement Learning","Policy Optimization","Hybrid Model","Biopharmaceutical Manufacturing","Process Control"],"title":"Policy Optimization in Bayesian Network Hybrid Models of Biomanufacturing Processes","type":"publication"},{"authors":["Wei Xie","Beverly Kris Jaeger-Helton","Jared Auclair","Jinxiang Pei","Hua Zheng"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623632571,"objectID":"ef209685ce3c8bb1e2f6bd925ac0e0ae","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/xie-2020-stem/","publishdate":"2021-06-14T01:02:50.207041Z","relpermalink":"/hua.zheng/publication/xie-2020-stem/","section":"publication","summary":"","tags":["Education"],"title":"STEM Education and Industry Workforce Life-Long Training Platform Development to Faciliate Smart Biopharmaceutical Manufacturing 4.0-4","type":"publication"},{"authors":["Hua Zheng","Ilya O. Ryzhov","Wei Xie","Judy Zhong"],"categories":["Deep Q-Netowrk","Electronic Health Records","Multimorbidity Management"],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623631605,"objectID":"462def7903cf843d8f6982b2042a57fe","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021/","publishdate":"2021-06-14T00:46:45.752236Z","relpermalink":"/hua.zheng/publication/zheng-2021/","section":"publication","summary":"BACKGROUND: Comorbid chronic conditions are common among people with type 2 diabetes. We developed an artificial intelligence algorithm, based on reinforcement learning (RL), for personalized diabetes and multimorbidity management, with strong potential to improve health outcomes relative to current clinical practice. METHODS: We modeled glycemia, blood pressure, and cardiovascular disease (CVD) risk as health outcomes, using a retrospective cohort of 16,665 patients with type 2 diabetes from New York University Langone Health ambulatory care electronic health records in 2009-2017. We trained an RL prescription algorithm that recommends a treatment regimen optimizing patients' cumulative health outcomes using their individual characteristics and medical history at each encounter. The RL recommendations were evaluated on an independent subset of patients. RESULTS: The single-outcome optimization RL algorithms, RL-glycemia, RL-blood pressure, and RL-CVD, recommended consistent prescriptions as that observed by clinicians in 86.1%, 82.9%, and 98.4% of the encounters, respectively. For patient encounters in which the RL recommendations differed from the clinician prescriptions, significantly fewer encounters showed uncontrolled glycemia (A1c þinspace8% in 35% of encounters), uncontrolled hypertension (blood pressure þinspace140 mmHg in 16% of encounters), and high CVD risk (risk þinspace20% in 25% of encounters) under RL algorithms compared with those observed under clinicians (43%, 27%, and 31% of encounters, respectively; all p ","tags":["Reinforcement Learning","Medical Research"],"title":"Personalized Multimorbidity Management for Patients with Type 2 Diabetes Using Reinforcement Learning of Electronic Health Records","type":"publication"},{"authors":null,"categories":null,"content":"","date":1613001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613001600,"objectID":"a836bc97c94ce94c8b202e5914d8cf39","permalink":"https://zhenghuazx.github.io/hua.zheng/project/ehr-rl/","publishdate":"2021-02-11T00:00:00Z","relpermalink":"/hua.zheng/project/ehr-rl/","section":"project","summary":"Personalized multimorbidity management for patients with type 2 diabetes using reinforcement learning of electronic health records.","tags":["Reinforcement Learning","Medical Research"],"title":"EHR-RL","type":"project"},{"authors":["Hua Zheng","Wei Xie","Ilya O Ryzhov","Dongming Xie"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623631697,"objectID":"ceb7b52aadfba46c23881dc2fed69210","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-policy/","publishdate":"2021-06-14T00:48:17.46984Z","relpermalink":"/hua.zheng/publication/zheng-2021-policy/","section":"publication","summary":"Biopharmaceutical manufacturing is a rapidly growing industry with impact in virtually all branches of medicine. Biomanufacturing processes require close monitoring and control, in the presence of complex bioprocess dynamics with many interdependent factors, as well as extremely limited data due to the high cost and long duration of experiments. We develop a novel model-based reinforcement learning framework that can achieve human-level control in low-data environments. The model uses a probabilistic knowledge graph to capture causal interdependencies between factors in the underlying stochastic decision process, leveraging information from existing kinetic models from different unit operations while incorporating real-world experimental data. We then present a computationally efficient, provably convergent stochastic gradient method for policy optimization. Validation is conducted on a realistic application with a multi-dimensional, continuous state variable.","tags":["Hyrbid Model","Reinforcement Learning","Process Control"],"title":"Policy Optimization in Bayesian Network Hybrid Models of Biomanufacturing Processes","type":"publication"},{"authors":["Hua Zheng","Jiahao Zhu","Wei Xie","Judy Zhong"],"categories":["Reinforcement Learning"],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650586931,"objectID":"d7da928484a0765ddad3c4015e74f2b9","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/zheng-2021-reinforcement/","publishdate":"2022-04-22T00:22:11.634209Z","relpermalink":"/hua.zheng/publication/zheng-2021-reinforcement/","section":"publication","summary":"**Background:** Patients with severe Coronavirus disease 19 (COVID-19) typically require supplemental oxygen as an essential treatment. We developed a machine learning algorithm, based on deep Reinforcement Learning (RL), for continuous management of oxygen flow rate for critically ill patients under intensive care, which can identify the optimal personalized oxygen flow rate with strong potentials to reduce mortality rate relative to the current clinical practice. **Methods:** We modeled the oxygen flow trajectory of COVID-19 patients and their health outcomes as a Markov decision process. Based on individual patient characteristics and health status, an optimal oxygen control policy is learned by using deep deterministic policy gradient (DDPG) and real-time recommends the oxygen flow rate to reduce the mortality rate. We assessed the performance of proposed methods through cross validation by using a retrospective cohort of 1372 critically ill patients with COVID-19 from New York University Langone Health ambulatory care with electronic health records from April 2020 to January 2021. **Results:** The mean mortality rate under the RL algorithm is lower than the standard of care by 2.57% (95% CI: 2.08–3.06) reduction (P ","tags":["Reinforcement Learning","Medical Research","Process Control"],"title":"Reinforcement Learning Assisted Oxygen Therapy for COVID-19 Patients Under Intensive Care","type":"publication"},{"authors":["Hua Zheng","Wei Xie","M. Ben Feng"],"categories":null,"content":"","date":1608028200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608028200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://zhenghuazx.github.io/hua.zheng/talk/winter-simulation-conference-2020/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/hua.zheng/talk/winter-simulation-conference-2020/","section":"event","summary":"Green Simulation Assisted Reinforcement Learning with Model Risk for Biomanufacturing Learning and Control.","tags":["Hybrid Model","Reinforcement Learning"],"title":"Winter Simulation Conference 2020","type":"event"},{"authors":["Hua Zheng","Wei Xie","M. Ben Feng"],"categories":["Policy Optimization","Biopharmaceutical Manufacturing"],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623631792,"objectID":"72354c6044d8c7227d4e718e634bfe43","permalink":"https://zhenghuazx.github.io/hua.zheng/publication/10-5555-3466184-3466221/","publishdate":"2021-06-14T00:49:51.969067Z","relpermalink":"/hua.zheng/publication/10-5555-3466184-3466221/","section":"publication","summary":"Biopharmaceutical manufacturing faces critical challenges, including complexity, high variability, lengthy lead time, and limited historical data and knowledge of the underlying system stochastic process. To address these challenges, we propose a green simulation assisted model-based reinforcement learning to support process online learning and guide dynamic decision making. Basically, the process model risk is quantified by the posterior distribution. At any given policy, we predict the expected system response with prediction risk accounting for both inherent stochastic uncertainty and model risk. Then, we propose green simulation assisted reinforcement learning and derive the mixture proposal distribution of decision process and likelihood ratio based metamodel for the policy gradient, which can selectively reuse process trajectory outputs collected from previous experiments to increase the simulation data-efficiency, improve the policy gradient estimation accuracy, and speed up the search for the optimal policy. Our numerical study indicates that the proposed approach demonstrates the promising performance.","tags":["Green Simulation","Reinforcement Learning","Biopharmaceutical Manufacturing"],"title":"Green Simulation Assisted Reinforcement Learning with Model Risk for Biomanufacturing Learning and Control","type":"publication"},{"authors":null,"categories":null,"content":"","date":1521936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521936000,"objectID":"a72d3248e856c05acd99e1fba2ddbaf4","permalink":"https://zhenghuazx.github.io/hua.zheng/project/toxiccomment/","publishdate":"2018-03-25T00:00:00Z","relpermalink":"/hua.zheng/project/toxiccomment/","section":"project","summary":"A multi-headed DL model, capable of detecting different types of toxic comments like threats, obscenity, insults, and identity-based hate. It ranked 116 out of 4,539 teams with the private LB score 0.98689.","tags":["Deep Learning","Kaggle"],"title":"Toxic Comment Classification","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://zhenghuazx.github.io/hua.zheng/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/hua.zheng/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]